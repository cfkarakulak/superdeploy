---
# Monitoring Addon Deployment Tasks
# Deploys Prometheus and Grafana for project monitoring

- name: Create {{ addon_name }} directories
  file:
    path: "{{ item }}"
    state: directory
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0755'
  loop:
    - "{{ addon_base_path }}/{{ addon_name }}"
    - "{{ addon_base_path }}/{{ addon_name }}/prometheus"
    - "{{ addon_base_path }}/{{ addon_name }}/grafana"
    - "{{ addon_base_path }}/{{ addon_name }}/grafana/provisioning"
    - "{{ addon_base_path }}/{{ addon_name }}/grafana/provisioning/datasources"
    - "{{ addon_base_path }}/{{ addon_name }}/grafana/provisioning/dashboards"
    - "{{ addon_base_path }}/{{ addon_name }}/grafana/provisioning/dashboards/json"

- name: Initialize monitoring projects list
  set_fact:
    monitoring_projects: []
  
- name: Build monitoring projects from inventory with app targets
  set_fact:
    monitoring_projects: "{{ monitoring_projects + [{'name': item.split('-')[0], 'vms': {item.split('-')[1]: hostvars[item]['ansible_host']}, 'app_targets': [hostvars[item]['ansible_host'] + ':' + (hostvars[item]['app_port'] | default('8000'))]}] }}"
  loop: "{{ groups['all'] | select('match', '^[^orchestrator].*-.*-\\d+$') | list }}"
  when: 
    - groups['all'] is defined
    - item.split('-') | length >= 2

- name: Merge VMs and app targets for same project
  set_fact:
    monitoring_projects_merged: {}

- name: Group VMs and app targets by project
  set_fact:
    monitoring_projects_merged: "{{ monitoring_projects_merged | combine({item.name: {'name': item.name, 'vms': (monitoring_projects_merged[item.name].vms | default({})) | combine(item.vms), 'app_targets': (monitoring_projects_merged[item.name].app_targets | default([])) + item.app_targets, 'environment': 'production'}}) }}"
  loop: "{{ monitoring_projects }}"
  when: monitoring_projects | length > 0

- name: Convert to list
  set_fact:
    monitoring_projects: "{{ monitoring_projects_merged.values() | list }}"
  when: monitoring_projects_merged | length > 0

- name: Generate Prometheus configuration
  template:
    src: "{{ addon_path }}/prometheus.yml.j2"
    dest: "{{ addon_base_path }}/{{ addon_name }}/prometheus/prometheus.yml"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0644'
  vars:
    projects: "{{ monitoring_projects | default([]) }}"
  register: prometheus_config_result

# Config will be copied after container starts

- name: Generate Grafana datasource provisioning
  template:
    src: "{{ addon_path }}/grafana-datasources.yml.j2"
    dest: "{{ addon_base_path }}/{{ addon_name }}/grafana/provisioning/datasources/datasources.yml"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0644'
  vars:
    projects: "{{ monitoring_projects | default([]) }}"

- name: Generate Grafana dashboard provisioning
  template:
    src: "{{ addon_path }}/grafana-dashboards.yml.j2"
    dest: "{{ addon_base_path }}/{{ addon_name }}/grafana/provisioning/dashboards/dashboards.yml"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0644'
  vars:
    projects: "{{ monitoring_projects | default([]) }}"

- name: Copy pre-built Grafana dashboards
  copy:
    src: "{{ addon_path }}/dashboards/{{ item }}"
    dest: "{{ addon_base_path }}/{{ addon_name }}/grafana/provisioning/dashboards/json/{{ item }}"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0644'
  loop:
    - superdeploy-overview.json
    - project-detail.json
    - infrastructure.json
    - alerts.json
    - system-status.json
    - live-traffic.json
    - endpoint-analytics.json

- name: Copy Prometheus alert rules
  copy:
    src: "{{ addon_path }}/alert-rules.yml"
    dest: "{{ addon_base_path }}/{{ addon_name }}/prometheus/alert-rules.yml"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0644'

# Note: docker-compose.yml is already rendered by orchestration/addon-deployer/tasks/render-templates.yml
# This keeps the deployment logic clean and focused on service-specific tasks

- name: Render {{ addon_name }} environment file
  template:
    src: "{{ addon_path }}/templates/monitoring.env.j2"
    dest: "{{ addon_base_path }}/{{ addon_name }}/.env"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0600'
  when: addon_path is defined and addon_path | length > 0

- name: Stop existing services (with explicit project name)
  community.docker.docker_compose_v2:
    project_name: "{{ project_name }}"
    project_src: "{{ addon_base_path }}/{{ addon_name }}"
    state: absent
    remove_orphans: true
  become_user: "{{ superdeploy_user }}"
  register: compose_down_result
  failed_when: false
  ignore_errors: true

- name: Wait for Docker to complete cleanup
  command: sleep 2
  changed_when: false

- name: Verify container removal and force cleanup if needed
  shell: |
    set -e
    # Monitoring has 3 containers
    NAMES=("{{ project_name }}-{{ addon_name }}" "{{ project_name }}-prometheus" "{{ project_name }}-grafana")
    
    # Force remove by name and ID
    for CNAME in "${NAMES[@]}"; do
      docker rm -f "$CNAME" 2>/dev/null || true
      docker ps -aq --filter "name=$CNAME" 2>/dev/null | xargs -r docker rm -f 2>/dev/null || true
    done
    
    # Clean Docker metadata cache
    docker container prune -f >/dev/null 2>&1 || true
    sleep 1
    
    # Final verification
    FAILED=""
    for CNAME in "${NAMES[@]}"; do
      if docker ps -aq --filter "name=$CNAME" 2>/dev/null | grep -q .; then
        FAILED="$FAILED $CNAME"
      fi
    done
    
    if [ -n "$FAILED" ]; then
      echo "FATAL: Containers still exist:$FAILED"
      docker ps -a
      exit 1
    fi
    
    echo "Container cleanup verified"
  args:
    executable: /bin/bash
  become_user: "{{ superdeploy_user }}"
  register: verify_cleanup
  changed_when: false
  failed_when: verify_cleanup.rc != 0

- name: Start {{ addon_name }} services (with explicit project name)
  community.docker.docker_compose_v2:
    project_name: "{{ project_name }}"
    project_src: "{{ addon_base_path }}/{{ addon_name }}"
    state: present
    pull: "always"
    recreate: "always"
    remove_orphans: true
  become_user: "{{ superdeploy_user }}"
  register: monitoring_compose_result

- name: Wait for Prometheus to be ready
  wait_for:
    host: localhost
    port: "{{ PROMETHEUS_PORT | default(9090) }}"
    delay: 5
    timeout: 60
    state: started
  when: monitoring_compose_result.changed

- name: Wait for Grafana to be ready
  wait_for:
    host: localhost
    port: "{{ GRAFANA_PORT | default(3000) }}"
    delay: 5
    timeout: 60
    state: started
  when: monitoring_compose_result.changed

- name: Verify Prometheus is accessible
  uri:
    url: "http://localhost:{{ PROMETHEUS_PORT | default(9090) }}/-/healthy"
    status_code: 200
    timeout: 5
  register: prometheus_health
  until: prometheus_health.status == 200
  retries: 10
  delay: 3
  failed_when: prometheus_health.status != 200

- name: Verify Grafana is accessible
  uri:
    url: "http://localhost:{{ GRAFANA_PORT | default(3000) }}/api/health"
    status_code: 200
    timeout: 5
  register: grafana_health
  until: grafana_health.status == 200
  retries: 10
  delay: 3
  failed_when: grafana_health.status != 200

- name: Reload Prometheus configuration (config is volume-mounted)
  uri:
    url: "http://localhost:{{ PROMETHEUS_PORT | default(9090) }}/-/reload"
    method: POST
    status_code: 200
    timeout: 5
  when: prometheus_config_result.changed
  register: prometheus_reload_result
  failed_when: false
  
- name: Restart Prometheus if reload failed
  community.docker.docker_compose_v2:
    project_src: "{{ addon_base_path }}/{{ addon_name }}"
    services:
      - prometheus
    state: restarted
  become_user: "{{ superdeploy_user }}"
  when: 
    - prometheus_config_result.changed
    - prometheus_reload_result.status is defined
    - prometheus_reload_result.status != 200

- name: Check if Caddy is enabled for domain access
  set_fact:
    caddy_enabled: "{{ 'caddy' in enabled_addons | default([]) }}"
    grafana_domain: "{{ addon_configs.monitoring.grafana_domain | default('') }}"
    prometheus_domain: "{{ addon_configs.monitoring.prometheus_domain | default('') }}"

- name: Display {{ addon_name }} deployment status
  debug:
    msg:
      - "Grafana Direct: http://{{ ansible_host }}:{{ GRAFANA_PORT | default(3000) }}"
      - "Prometheus Direct: http://{{ ansible_host }}:{{ PROMETHEUS_PORT | default(9090) }}"
      - "Grafana Admin User: {{ GRAFANA_ADMIN_USER }}"
      - "{% if caddy_enabled and grafana_domain %}Grafana Domain: https://{{ grafana_domain }}{% endif %}"
      - "{% if caddy_enabled and prometheus_domain %}Prometheus Domain: https://{{ prometheus_domain }}{% endif %}"
      - "{% if caddy_enabled %}Note: Domain access requires DNS records and Caddy deployment{% endif %}"
