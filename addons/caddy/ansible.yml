---
# Caddy Addon Deployment Tasks
# Handles deployment and certificate management for Caddy reverse proxy

# Check if this is orchestrator deployment
- name: Check if orchestrator deployment
  set_fact:
    is_orchestrator: "{{ project_name == 'orchestrator' }}"

# Include orchestrator-specific tasks if needed
- name: Include orchestrator-specific Caddy deployment
  include_tasks: "{{ addon_path }}/ansible.orchestrator.yml"
  when: is_orchestrator | bool

# Skip regular deployment for orchestrator
- name: Skip regular Caddy deployment for orchestrator
  meta: end_play
  when: is_orchestrator | bool

# Regular project Caddy deployment continues below
- name: Create {{ addon_name }} directories
  file:
    path: "{{ item }}"
    state: directory
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0755'
  loop:
    - "{{ addon_base_path }}/{{ addon_name }}"
    - "{{ addon_base_path }}/{{ addon_name }}/data"
    - "{{ addon_base_path }}/{{ addon_name }}/config"

- name: Render Caddyfile for {{ addon_name }}
  template:
    src: "{{ addon_path }}/Caddyfile.j2"
    dest: "{{ addon_base_path }}/{{ addon_name }}/Caddyfile"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0644'
  register: caddyfile_result

# Note: docker-compose.yml is already rendered by orchestration/addon-deployer/tasks/render-templates.yml
# This keeps the deployment logic clean and focused on service-specific tasks

- name: Render {{ addon_name }} environment file
  template:
    src: "{{ addon_path }}/templates/caddy.env.j2"
    dest: "{{ addon_base_path }}/{{ addon_name }}/.env"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0600'
  when: addon_path is defined and addon_path | length > 0

- name: CLEANUP - Nuclear option container removal
  shell: |
    set -ex
    FILTER_NAME="{{ project_name }}-{{ addon_name }}"
    
    echo "========== CLEANUP START =========="
    echo "Target: ${FILTER_NAME}"
    echo "User: $(whoami)"
    docker ps -a | grep "${FILTER_NAME}" || echo "No containers found initially"
    echo "==================================="
    
    # Find all matching containers
    CONTAINERS=$(docker ps -aq --filter "name=${FILTER_NAME}" 2>/dev/null || echo "")
    
    if [ -n "$CONTAINERS" ]; then
      echo ">>> Found containers to clean: $CONTAINERS"
      
      # Kill all
      for CID in $CONTAINERS; do
        echo "Killing $CID..."
        docker kill "$CID" 2>&1 || echo "Failed to kill (already stopped?)"
        docker stop -t 0 "$CID" 2>&1 || echo "Failed to stop"
      done
      
      # Disconnect networks
      for CID in $CONTAINERS; do
        NETWORKS=$(docker inspect "$CID" --format '{% raw %}{{range $net, $_ := .NetworkSettings.Networks}}{{$net}} {{end}}{% endraw %}' 2>/dev/null || echo "")
        for NET in $NETWORKS; do
          echo "Disconnecting $CID from $NET..."
          docker network disconnect -f "$NET" "$CID" 2>&1 || echo "Failed to disconnect"
        done
      done
      
      # Remove all
      for CID in $CONTAINERS; do
        echo "Removing $CID..."
        docker rm -fv "$CID" 2>&1 || echo "Failed to remove"
      done
    else
      echo ">>> No containers found by filter"
    fi
    
    # Try by name 5 times
    for i in {1..5}; do
      echo "Attempt $i: Removing by name..."
      docker kill "${FILTER_NAME}" 2>&1 || true
      docker stop -t 0 "${FILTER_NAME}" 2>&1 || true
      docker rm -fv "${FILTER_NAME}" 2>&1 || true
    done
    
    # Prune
    echo "Pruning containers..."
    docker container prune -f --filter "label=com.docker.compose.project={{ project_name }}" 2>&1 || true
    
    sleep 2
    
    # Check if any remain
    REMAINING=$(docker ps -aq --filter "name=${FILTER_NAME}" 2>/dev/null || echo "")
    
    if [ -n "$REMAINING" ]; then
      echo "!!! REMAINING CONTAINERS: $REMAINING"
      docker system prune -af --volumes --filter "label=com.docker.compose.service={{ addon_name }}" 2>&1 || true
      sleep 1
      STILL_REMAINING=$(docker ps -aq --filter "name=${FILTER_NAME}" 2>/dev/null || echo "")
      if [ -n "$STILL_REMAINING" ]; then
        echo "!!! CLEANUP FAILED - containers still exist: $STILL_REMAINING"
        docker ps -a | grep "${FILTER_NAME}"
        exit 1
      fi
    fi
    
    echo "========== CLEANUP SUCCESS =========="
    docker ps -a | grep "${FILTER_NAME}" || echo "No containers remain - SUCCESS!"
    echo "====================================="
  args:
    executable: /bin/bash
  become_user: "{{ superdeploy_user }}"
  register: cleanup_result
  changed_when: false
  failed_when: cleanup_result.rc != 0

- name: DEBUG - Show cleanup output
  debug:
    msg: "{{ cleanup_result.stdout_lines }}"
  when: cleanup_result.stdout_lines is defined

- name: Start {{ addon_name }} services (fresh deployment)
  community.docker.docker_compose_v2:
    project_src: "{{ addon_base_path }}/{{ addon_name }}"
    state: present
    pull: "always"
    recreate: "always"
    remove_orphans: true
  become_user: "{{ superdeploy_user }}"
  register: caddy_compose_result

- name: Wait for {{ addon_name }} to be ready
  wait_for:
    host: localhost
    port: "{{ CADDY_ADMIN_PORT | default(2019) }}"
    delay: 5
    timeout: 30
    state: started
  when: caddy_compose_result.changed

- name: Verify {{ addon_name }} admin API is accessible
  uri:
    url: "http://localhost:{{ CADDY_ADMIN_PORT | default(2019) }}/config/"
    method: GET
    status_code: 200
  register: caddy_health
  until: caddy_health.status == 200
  retries: 5
  delay: 3
  failed_when: false

- name: Verify {{ addon_name }} TLS certificate status
  uri:
    url: "http://localhost:{{ CADDY_ADMIN_PORT | default(2019) }}/config/apps/tls/certificates"
    method: GET
    status_code: 200
  register: caddy_certs
  failed_when: false

- name: Display {{ addon_name }} certificate information
  debug:
    msg: "Caddy TLS certificates: {{ caddy_certs.json | default('Not available') }}"
  when: 
    - caddy_certs is defined
    - caddy_certs.json is defined

- name: Ensure {{ addon_name }} certificate renewal is configured
  cron:
    name: "Check Caddy certificate renewal for {{ project_name }}"
    minute: "0"
    hour: "2"
    job: "docker exec {{ project_name }}-{{ addon_name }} caddy reload --config /etc/caddy/Caddyfile"
    user: "{{ superdeploy_user }}"

- name: Display {{ addon_name }} deployment status
  debug:
    msg: "Caddy is ready - HTTP on port {{ CADDY_HTTP_PORT | default(80) }}, HTTPS on port {{ CADDY_HTTPS_PORT | default(443) }}"
