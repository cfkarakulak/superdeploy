---
# Caddy Addon Instance Deployment Tasks
# Instance: {{ instance_full_name }}
# Handles deployment and certificate management for Caddy reverse proxy

# Check if this is orchestrator deployment
- name: Check if orchestrator deployment
  set_fact:
    is_orchestrator: "{{ project_name == 'orchestrator' }}"

# Include orchestrator-specific tasks if needed
- name: Include orchestrator-specific Caddy deployment
  include_tasks: "{{ addon_path }}/ansible.orchestrator.yml"
  when: is_orchestrator | bool

# Skip regular deployment for orchestrator
- name: Skip regular Caddy deployment for orchestrator
  meta: end_play
  when: is_orchestrator | bool

# Regular project Caddy deployment continues below
- name: Create {{ instance_name }} directories
  file:
    path: "{{ item }}"
    state: directory
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0755'
  loop:
    - "{{ addon_deployment_path }}"
    - "{{ addon_deployment_path }}/data"
    - "{{ addon_deployment_path }}/config"

- name: Render Caddyfile for {{ instance_name }}
  template:
    src: "{{ addon_path }}/Caddyfile.j2"
    dest: "{{ addon_deployment_path }}/Caddyfile"
    owner: "{{ superdeploy_user }}"
    group: "{{ superdeploy_group | default(superdeploy_user) }}"
    mode: '0644'
  register: caddyfile_result

# Note: docker-compose.yml and .env are already rendered by orchestration/addon-deployer/tasks/
# This keeps the deployment logic clean and focused on service-specific tasks

- name: Deploy {{ instance_name }} with intelligent cleanup
  shell:
    cmd: |
      set -e
      cd "${ADDON_DEPLOYMENT_PATH}"
      
      echo "Starting cleanup for ${CONTAINER_NAME}..."
      
      # Phase 1: Stop via Docker Compose (graceful shutdown with remove)
      echo "Phase 1: Stopping containers via Docker Compose..."
      docker compose --project-name "${CONTAINER_NAME}" down --remove-orphans --volumes --timeout 30 2>&1 || true
      
      # Phase 2: Wait for Docker daemon to sync
      echo "Phase 2: Waiting for Docker daemon to sync..."
      sleep 5
      
      # Force sync by listing containers
      docker ps -a > /dev/null 2>&1
      
      # Phase 3: Force remove any remaining containers by exact name or prefix
      echo "Phase 3: Force removing any remaining containers..."
      # Get all container names matching our pattern and filter for exact/prefix match
      ALL_NAMES=$(docker ps -a --filter "name=${CONTAINER_NAME}" --format '{{ '{{' }}.Names{{ '}}' }}' 2>/dev/null || true)
      EXACT_CONTAINER=""
      PREFIX_CONTAINERS=""
      
      for name in $ALL_NAMES; do
        if [ "$name" = "${CONTAINER_NAME}" ]; then
          # Exact match
          EXACT_CONTAINER=$(docker ps -aq --filter "name=${name}" 2>/dev/null || true)
        elif [[ "$name" == ${CONTAINER_NAME}-* ]]; then
          # Prefix match (compose generated names)
          PREFIX_CONTAINERS="$PREFIX_CONTAINERS $(docker ps -aq --filter "name=${name}" 2>/dev/null || true)"
        fi
      done
      
      REMAINING_CONTAINERS=$(echo "$EXACT_CONTAINER $PREFIX_CONTAINERS" | tr -s ' ' | xargs)
      
      if [ -n "$REMAINING_CONTAINERS" ]; then
        echo "Found remaining containers: $REMAINING_CONTAINERS"
        # First stop, then remove
        echo "$REMAINING_CONTAINERS" | xargs -r docker stop -t 5 2>&1 || true
        echo "$REMAINING_CONTAINERS" | xargs -r docker rm -f 2>&1 || true
        sleep 5
      
      # Force sync by listing containers
      docker ps -a > /dev/null 2>&1
      fi
      
      # Phase 4: Final verification
      echo "Phase 4: Final verification..."
      ALL_NAMES_FINAL=$(docker ps -a --filter "name=${CONTAINER_NAME}" --format '{{ '{{' }}.Names{{ '}}' }}' 2>/dev/null || true)
      EXACT_CHECK=""
      PREFIX_CHECK=""
      
      for name in $ALL_NAMES_FINAL; do
        if [ "$name" = "${CONTAINER_NAME}" ]; then
          EXACT_CHECK=$(docker ps -aq --filter "name=${name}" 2>/dev/null || true)
        elif [[ "$name" == ${CONTAINER_NAME}-* ]]; then
          PREFIX_CHECK="$PREFIX_CHECK $(docker ps -aq --filter "name=${name}" 2>/dev/null || true)"
        fi
      done
      
      FINAL_CHECK=$(echo "$EXACT_CHECK $PREFIX_CHECK" | tr -s ' ' | xargs)
      
      if [ -n "$FINAL_CHECK" ]; then
        echo "‚ö† WARNING: Some containers still exist:"
        docker ps -a | grep "${CONTAINER_NAME}" || true
        echo "Attempting final force removal..."
        # Stop first, then remove
        echo "$FINAL_CHECK" | xargs -r docker stop -t 5 2>&1 || true
        echo "$FINAL_CHECK" | xargs -r docker rm -f 2>&1 || true
        sleep 5
      
      # Force sync by listing containers
      docker ps -a > /dev/null 2>&1
      fi
      
      echo "‚úì Cleanup completed - ready for deployment"
    executable: /bin/bash
  environment:
    CONTAINER_NAME: "{{ container_name }}"
    PROJECT_NAME: "{{ project_name }}"
    ADDON_NAME: "{{ instance_name }}"
    ADDON_DEPLOYMENT_PATH: "{{ addon_deployment_path }}"
    ADDON_BASE_PATH: "{{ addons_base_path }}"
  become: yes
  register: caddy_cleanup_result
  changed_when: false

- name: Display cleanup output (debug)
  debug:
    msg: "{{ caddy_cleanup_result.stdout_lines }}"
  when: caddy_cleanup_result.stdout_lines is defined

- name: Final pre-start container check for {{ instance_name }}
  shell:
    cmd: |
      set -x  # Debug mode
      echo "=== Pre-start check for {{ project_name }}-{{ instance_name }} ==="
      # One last check before docker-compose starts
      EXISTING=$(docker ps -aq --filter "name={{ project_name }}-{{ instance_name }}" 2>/dev/null || true)
      echo "Found containers: [$EXISTING]"
      if [ -n "$EXISTING" ]; then
        echo "Force removing existing container(s)..."
        echo "$EXISTING" | xargs -r docker stop -t 5 2>&1 || true
        echo "$EXISTING" | xargs -r docker rm -f 2>&1 || true
        sleep 3
        docker ps -a > /dev/null 2>&1  # Force sync
        echo "‚úì Removed: $EXISTING"
      else
        echo "‚úì No existing containers found - clean state"
      fi
    executable: /bin/bash
  become: yes
  register: prestart_check_result
  changed_when: false

- name: Display pre-start check output
  debug:
    msg: "{{ prestart_check_result.stdout_lines }}"
  when: prestart_check_result.stdout_lines is defined

- name: Force remove all {{ instance_name }} containers with shell (bypass Docker SDK bug)
  shell:
    cmd: |
      echo "=== Force removing {{ project_name }}-{{ instance_name }} containers ==="
      # Get ALL matching containers (both exact and prefix)
      ALL_CONTAINERS=$(docker ps -aq --filter "name={{ project_name }}-{{ instance_name }}" 2>/dev/null || true)
      if [ -n "$ALL_CONTAINERS" ]; then
        echo "Found containers: $ALL_CONTAINERS"
        # Stop first
        echo "$ALL_CONTAINERS" | xargs -r docker stop -t 5 2>&1 || true
        # Then remove
        echo "$ALL_CONTAINERS" | xargs -r docker rm -f 2>&1 || true
        sleep 3
        docker ps -a > /dev/null 2>&1  # Force sync
        echo "‚úì Removed all containers"
      else
        echo "‚úì No containers to remove"
      fi
    executable: /bin/bash
  become: yes
  changed_when: false

- name: Start {{ instance_name }} services (with explicit project name)
  community.docker.docker_compose_v2:
    project_name: "{{ project_name }}-{{ instance_name }}"
    project_src: "{{ addon_deployment_path }}"
    state: present
    pull: "always"
    recreate: "always"
    remove_orphans: true
    build: never
  become: yes
  register: caddy_compose_result
  # If this fails with "container name already in use", the cleanup phase didn't work
  # This should be extremely rare now with the 5-attempt verification loop above
  failed_when:
    - caddy_compose_result.failed
    - '"already in use" in caddy_compose_result.msg | default("")'

- name: Wait for {{ instance_name }} container to start
  shell: sleep 5
  changed_when: false
  when: caddy_compose_result.changed

- name: Wait for {{ instance_name }} to be ready
  wait_for:
    host: localhost
    port: "{{ ADMIN_PORT | default(2019) }}"
    delay: 2
    timeout: 60
    state: started

- name: Verify {{ instance_name }} admin API is accessible
  uri:
    url: "http://localhost:{{ ADMIN_PORT | default(2019) }}/config/"
    method: GET
    status_code: 200
  register: caddy_health
  until: caddy_health.status == 200
  retries: 5
  delay: 3
  failed_when: false

- name: Verify {{ instance_name }} TLS certificate status
  uri:
    url: "http://localhost:{{ ADMIN_PORT | default(2019) }}/config/apps/tls/certificates"
    method: GET
    status_code: 200
  register: caddy_certs
  failed_when: false

- name: Display {{ instance_name }} certificate information
  debug:
    msg: "Caddy TLS certificates: {{ caddy_certs.json | default('Not available') }}"
  when: 
    - caddy_certs is defined
    - caddy_certs.json is defined

- name: Ensure {{ instance_name }} certificate renewal is configured
  cron:
    name: "Check Caddy certificate renewal for {{ project_name }}"
    minute: "0"
    hour: "2"
    job: "docker exec {{ container_name }} caddy reload --config /etc/caddy/Caddyfile"
    user: "{{ superdeploy_user }}"

- name: "üåê  Caddy ready ‚Üí HTTP: {{ HTTP_PORT | default(80) }} ‚Ä¢ HTTPS: {{ HTTPS_PORT | default(443) }} ‚Ä¢ TLS: Auto (Let's Encrypt) ‚Ä¢ LB: Round-robin"
  set_fact:
    _caddy_ready: true
  changed_when: false
