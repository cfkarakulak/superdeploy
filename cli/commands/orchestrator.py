"""SuperDeploy CLI - Orchestrator command (with improved logging and UX)"""

import click
import subprocess
import time
from rich.console import Console
from cli.ui_components import show_header
from rich.prompt import Prompt
from cli.utils import get_project_root
from cli.logger import DeployLogger

console = Console()


@click.command(name="orchestrator:init")
def orchestrator_init():
    """Initialize orchestrator configuration"""
    import yaml

    show_header(
        title="Orchestrator Setup",
        subtitle="Global monitoring (Prometheus + Grafana)",
        show_logo=True,
        console=console,
    )

    project_root = get_project_root()
    shared_dir = project_root / "shared"
    orchestrator_dir = shared_dir / "orchestrator"
    orchestrator_dir.mkdir(parents=True, exist_ok=True)

    config_path = orchestrator_dir / "config.yml"

    # Check if config already exists
    if config_path.exists():
        console.print(
            "[yellow]Config exists. Overwrite? [y/n][/yellow] [dim](n)[/dim]: ",
            end="",
        )
        answer = input().strip().lower()
        console.print()  # Add newline after input
        if answer not in ["y", "yes"]:
            console.print("[dim]Cancelled[/dim]")
            return

    # Cloud Configuration
    console.print("\n[white]Cloud Configuration[/white]")
    gcp_project = Prompt.ask("GCP Project ID", default="")
    region = Prompt.ask("GCP Region", default="us-central1")
    zone = Prompt.ask("GCP Zone", default=f"{region}-a")

    # SSL Configuration
    console.print("\n[white]SSL Configuration[/white]")
    ssl_email = Prompt.ask("Email for Let's Encrypt", default="")

    # Allocate Docker subnet for orchestrator
    console.print("\n[dim]Allocating network subnet...[/dim]")
    from cli.subnet_allocator import SubnetAllocator

    allocator = SubnetAllocator()

    # Check if orchestrator already has a subnet allocated
    if "orchestrator" not in allocator.docker_allocations:
        docker_subnet = SubnetAllocator.ORCHESTRATOR_DOCKER_SUBNET
        allocator.docker_allocations["orchestrator"] = docker_subnet
        allocator.allocations["docker_subnets"] = allocator.docker_allocations
        allocator._save_allocations()
    else:
        docker_subnet = allocator.docker_allocations["orchestrator"]

    console.print(f"[dim]‚úì Subnet allocated: {docker_subnet}[/dim]")

    # Build configuration dictionary
    orchestrator_config = {
        "project": {
            "name": "orchestrator",
            "ssl_email": ssl_email,
        },
        "gcp": {
            "project_id": gcp_project,
            "region": region,
            "zone": zone,
        },
        "ssh": {
            "key_path": "~/.ssh/superdeploy_deploy",
            "public_key_path": "~/.ssh/superdeploy_deploy.pub",
            "user": "superdeploy",
        },
        "vm": {
            "name": "orchestrator",
            "machine_type": "e2-medium",
            "disk_size": 50,
        },
        "network": {
            "vpc_name": "superdeploy-network",
            "subnet_cidr": "10.128.0.0/20",
            "docker_subnet": docker_subnet,
        },
        "grafana": {
            "domain": "",
            "port": 3000,
            "smtp_enabled": False,
            "smtp_host": "",
            "smtp_user": "",
        },
        "prometheus": {
            "domain": "",
            "port": 9090,
            "retention": "15d",
        },
        "caddy": {
            "enabled": False,
            "version": "2-alpine",
            "http_port": 80,
            "https_port": 443,
            "admin_port": 2019,
        },
    }

    # Build formatted YAML with comments
    yml_lines = []
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# Orchestrator Configuration")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# Auto-generated by: superdeploy orchestrator init")
    yml_lines.append("# Then run: superdeploy orchestrator up")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# Project Information")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append(
        yaml.dump(
            {"project": orchestrator_config["project"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append("")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# Cloud Provider Configuration")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append(
        yaml.dump(
            {"gcp": orchestrator_config["gcp"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append("")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# SSH Configuration")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append(
        yaml.dump(
            {"ssh": orchestrator_config["ssh"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append("")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# VM Configuration")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append(
        yaml.dump(
            {"vm": orchestrator_config["vm"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append("")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# Network Configuration")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append(
        yaml.dump(
            {"network": orchestrator_config["network"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append("")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# Monitoring Configuration (Global - All Projects)")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append(
        "# Grafana and Prometheus run on orchestrator VM and monitor ALL projects"
    )
    yml_lines.append(
        "# Configure a single domain for global access (e.g., grafana.yourdomain.com)"
    )
    yml_lines.append("# All projects will be visible in Grafana with project filtering")
    yml_lines.append("")
    yml_lines.append("# Grafana Configuration")
    yml_lines.append(
        yaml.dump(
            {"grafana": orchestrator_config["grafana"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append(
        "# Alert notification settings (optional - configure in Grafana UI)"
    )
    yml_lines.append(
        "# Variables: GRAFANA_ADMIN_USER, GRAFANA_ADMIN_PASSWORD, GRAFANA_SMTP_PASSWORD (from secrets.yml)"
    )
    yml_lines.append("")
    yml_lines.append("# Prometheus Configuration")
    yml_lines.append(
        yaml.dump(
            {"prometheus": orchestrator_config["prometheus"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append("")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append("# Caddy Configuration (Reverse Proxy for HTTPS)")
    yml_lines.append("# " + "=" * 77)
    yml_lines.append(
        yaml.dump(
            {"caddy": orchestrator_config["caddy"]},
            default_flow_style=False,
            sort_keys=False,
        ).strip()
    )
    yml_lines.append("")

    # Write config
    with open(config_path, "w") as f:
        f.write("\n".join(yml_lines))

    console.print(
        f"\n[dim]‚úì Config saved: {config_path.relative_to(project_root)}[/dim]"
    )
    console.print("\n[white]Next steps:[/white]")
    console.print("  [dim]1. Review: shared/orchestrator/config.yml[/dim]")
    console.print("  [dim]2. Deploy: superdeploy orchestrator up[/dim]\n")


@click.command(name="orchestrator:down")
@click.option("--yes", "-y", is_flag=True, help="Skip confirmation prompt")
@click.option("--preserve-ip", is_flag=True, help="Keep static IP (don't delete)")
@click.option("--verbose", "-v", is_flag=True, help="Show all command output")
def orchestrator_down(yes, preserve_ip, verbose):
    """Destroy orchestrator VM and clean up state"""

    project_root = get_project_root()

    # Initialize logger
    logger = DeployLogger("orchestrator", "down", verbose=verbose)

    if not verbose:
        show_header(
            title="Orchestrator Destruction",
            subtitle="[bold red]This will destroy the orchestrator VM and clean up all state![/bold red]",
            console=console,
        )

    if not yes:
        console.print(
            "[bold red]Are you sure you want to destroy the orchestrator?[/bold red] "
            "[bold bright_white]\\[y/n][/bold bright_white] [dim](n)[/dim]: ",
            end="",
        )
        answer = input().strip().lower()
        console.print()  # Add newline after input
        confirmed = answer in ["y", "yes"]

        if not confirmed:
            console.print("[yellow]‚ùå Destruction cancelled[/yellow]")
            logger.log("User cancelled destruction")
            return

    shared_dir = project_root / "shared"

    from cli.core.orchestrator_loader import OrchestratorLoader

    orchestrator_loader = OrchestratorLoader(shared_dir)

    # Try to load config, but don't fail if it doesn't exist (for cleanup)
    try:
        orch_config = orchestrator_loader.load()
        gcp_config = orch_config.config.get("gcp", {})
        zone = gcp_config.get("zone", "us-central1-a")
        region = gcp_config.get("region", "us-central1")
    except FileNotFoundError:
        # Config doesn't exist - use defaults for cleanup
        # This is OK for down command since we're destroying everything anyway
        logger.log("[dim]No config found, using defaults for cleanup[/dim]")
        zone = "us-central1-a"
        region = "us-central1"

    # Always do manual cleanup FIRST to ensure GCP resources are gone
    current_step = 1
    total_steps = 3

    logger.step(f"[{current_step}/{total_steps}] GCP Resource Cleanup")
    console.print("  [dim]‚úì Configuration loaded[/dim]")

    vms_deleted = 0
    ips_deleted = 0
    firewalls_deleted = 0
    subnets_deleted = 0
    networks_deleted = 0

    # Delete VM instances first
    result = subprocess.run(
        "gcloud compute instances list --filter='name:orchestrator-*' --format='value(name,zone)' 2>/dev/null",
        shell=True,
        capture_output=True,
        text=True,
    )

    if result.returncode == 0 and result.stdout.strip():
        lines = result.stdout.strip().split("\n")
        for line in lines:
            if line.strip():
                parts = line.split()
                if len(parts) >= 2:
                    vm_name, vm_zone = parts[0], parts[1]
                    result = subprocess.run(
                        f"gcloud compute instances delete {vm_name} --zone={vm_zone} --quiet",
                        shell=True,
                        capture_output=True,
                    )
                    if result.returncode == 0:
                        vms_deleted += 1

    # Delete External IP (unless --preserve-ip flag is set)
    if not preserve_ip:
        result = subprocess.run(
            "gcloud compute addresses list --filter='name:orchestrator-*' --format='value(name,region)' 2>/dev/null",
            shell=True,
            capture_output=True,
            text=True,
        )

        if result.returncode == 0 and result.stdout.strip():
            lines = result.stdout.strip().split("\n")
            for line in lines:
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 2:
                        ip_name, ip_region = parts[0], parts[1]
                        result = subprocess.run(
                            f"gcloud compute addresses delete {ip_name} --region={ip_region} --quiet",
                            shell=True,
                            capture_output=True,
                        )
                        if result.returncode == 0:
                            ips_deleted += 1

    # Delete Firewall Rules
    result = subprocess.run(
        "gcloud compute firewall-rules list --filter='network:superdeploy-network' --format='value(name)' 2>/dev/null",
        shell=True,
        capture_output=True,
        text=True,
    )

    if result.returncode == 0 and result.stdout.strip():
        firewall_rules = result.stdout.strip().split("\n")
        for rule in firewall_rules:
            rule = rule.strip()
            if rule:
                result = subprocess.run(
                    f"gcloud compute firewall-rules delete {rule} --quiet",
                    shell=True,
                    capture_output=True,
                )
                if result.returncode == 0:
                    firewalls_deleted += 1

    # Delete Subnet
    result = subprocess.run(
        f"gcloud compute networks subnets delete superdeploy-network-subnet --region={region} --quiet 2>&1",
        shell=True,
        capture_output=True,
        text=True,
    )
    if result.returncode == 0 or "not found" in result.stderr.lower():
        subnets_deleted += 1

    # Delete Network
    result = subprocess.run(
        "gcloud compute networks delete superdeploy-network --quiet 2>&1",
        shell=True,
        capture_output=True,
        text=True,
    )
    if result.returncode == 0 or "not found" in result.stderr.lower():
        networks_deleted += 1

    # Show summary
    resources = []
    if vms_deleted > 0:
        resources.append(f"{vms_deleted} VM(s)")
    if firewalls_deleted > 0:
        resources.append(f"{firewalls_deleted} firewall rule(s)")
    if subnets_deleted > 0:
        resources.append(f"{subnets_deleted} subnet(s)")
    if networks_deleted > 0:
        resources.append(f"{networks_deleted} network(s)")
    if ips_deleted > 0:
        resources.append(f"{ips_deleted} IP(s)")

    if resources:
        console.print(f"  [dim]‚úì GCP resources cleaned: {', '.join(resources)}[/dim]")
    else:
        console.print("  [dim]‚úì No GCP resources found[/dim]")

    # Now try Terraform cleanup
    terraform_success = False
    terraform_dir = shared_dir / "terraform"
    workspace_found = workspace_exists("orchestrator")

    current_step += 1
    logger.step(f"[{current_step}/{total_steps}] Terraform State Cleanup")

    if workspace_found:
        # Switch to default workspace
        subprocess.run(
            "terraform workspace select default 2>/dev/null || true",
            shell=True,
            cwd=terraform_dir,
            capture_output=True,
        )

        # Just remove the workspace directory directly since resources are already deleted from GCP
        terraform_state_dir = terraform_dir / "terraform.tfstate.d" / "orchestrator"
        if terraform_state_dir.exists():
            import shutil

            shutil.rmtree(terraform_state_dir)
            console.print("  [dim]‚úì Terraform workspace cleaned[/dim]")
    else:
        console.print("  [dim]‚úì No Terraform workspace found[/dim]")

    # Final cleanup step
    current_step += 1
    logger.step(f"[{current_step}/{total_steps}] Local Files Cleanup")

    # Delete state.yml completely
    state_file = shared_dir / "orchestrator" / "state.yml"
    if state_file.exists():
        state_file.unlink()

    # Clean inventory
    inventory_file = shared_dir / "ansible" / "inventories" / "orchestrator.ini"
    if inventory_file.exists():
        inventory_file.unlink()

    # Release subnet allocation
    try:
        from cli.subnet_allocator import SubnetAllocator

        allocator = SubnetAllocator()
        allocator.release_subnet("orchestrator")
    except Exception as e:
        logger.warning(f"Subnet release warning: {e}")

    console.print("  [dim]‚úì Local files cleaned[/dim]")

    console.print("\n[color(248)]Orchestrator destroyed.[/color(248)]")


@click.command(name="orchestrator:status")
def orchestrator_status():
    """Show orchestrator status"""
    project_root = get_project_root()
    shared_dir = project_root / "shared"

    from cli.core.orchestrator_loader import OrchestratorLoader

    orchestrator_loader = OrchestratorLoader(shared_dir)

    try:
        orch_config = orchestrator_loader.load()
    except FileNotFoundError as e:
        console.print(f"[red]‚ùå {e}[/red]")
        raise SystemExit(1)

    if orch_config.is_deployed():
        console.print("[green]‚úÖ Orchestrator is deployed[/green]")
        console.print(f"  IP: {orch_config.get_ip()}")
        console.print(f"  URL: http://{orch_config.get_ip()}:3001")

        # Get state details
        state = orch_config.state_manager.load_state()
        last_updated = state.get("last_updated", "Unknown")
        vm_info = state.get("vm", {})

        console.print(f"  Last Updated: {last_updated}")
        if vm_info.get("machine_type"):
            console.print(f"  VM Type: {vm_info.get('machine_type')}")
        if vm_info.get("disk_size"):
            console.print(f"  Disk Size: {vm_info.get('disk_size')}GB")
    else:
        console.print("[yellow]‚ö†Ô∏è  Orchestrator not deployed[/yellow]")
        console.print("  Run: [red]superdeploy orchestrator up[/red]")


@click.command(name="orchestrator:up")
@click.option(
    "--skip-terraform", is_flag=True, help="Skip Terraform (VM already exists)"
)
@click.option(
    "--preserve-ip", is_flag=True, help="Preserve static IP on destroy (for production)"
)
@click.option(
    "--addon",
    help="Deploy only specific addon(s), comma-separated (e.g. --addon monitoring,caddy)",
)
@click.option(
    "--tags", help="Run only specific Ansible tags (e.g. 'addons', 'foundation')"
)
@click.option(
    "--verbose",
    "-v",
    is_flag=True,
    help="Show all command output (default: clean UI with logs)",
)
@click.option(
    "--force",
    is_flag=True,
    help="Force deployment (ignore state, re-run everything)",
)
def orchestrator_up(skip_terraform, preserve_ip, addon, tags, verbose, force):
    """Deploy orchestrator VM with monitoring (runs Terraform + Ansible by default)"""

    if not verbose:
        show_header(
            title="Orchestrator Deployment",
            subtitle="Deploying monitoring infrastructure (Prometheus + Grafana)",
            show_logo=True,
            console=console,
        )

    project_root = get_project_root()
    shared_dir = project_root / "shared"

    # Initialize logger
    with DeployLogger("orchestrator", "up", verbose=verbose) as logger:
        try:
            _deploy_orchestrator(
                logger,
                project_root,
                shared_dir,
                skip_terraform,
                preserve_ip,
                addon,
                tags,
                verbose,
                force,
            )

            if not verbose:
                console.print("\n[color(248)]Orchestrator deployed.[/color(248)]")

        except Exception as e:
            logger.log_error(str(e), context="Orchestrator deployment failed")
            console.print(f"\n[dim]Logs saved to:[/dim] {logger.log_path}\n")
            raise SystemExit(1)


def _deploy_orchestrator(
    logger,
    project_root,
    shared_dir,
    skip_terraform,
    preserve_ip,
    addon,
    tags,
    verbose,
    force,
):
    """Internal function for orchestrator deployment with logging"""

    logger.step("[1/3] Setup & Infrastructure")

    # Load orchestrator config
    from cli.core.orchestrator_loader import OrchestratorLoader

    orchestrator_loader = OrchestratorLoader(shared_dir)

    try:
        orch_config = orchestrator_loader.load()
        console.print("  [dim]‚úì Configuration loaded[/dim]")
    except FileNotFoundError as e:
        logger.log_error(str(e), context="Orchestrator config not found")
        raise SystemExit(1)

    # Force mode: Clear state to trigger full re-deployment
    if force:
        state_file = shared_dir / "orchestrator" / "state.yml"
        if state_file.exists():
            state_file.unlink()
            logger.log("üóëÔ∏è  State cleared (force mode)")
            logger.log("")

    # Check state and detect changes (unless forced or specific addon)
    if not force and not addon:
        logger.log("Detecting changes...")

        # Simple state check for orchestrator (different from project state management)
        state = orch_config.state_manager.load_state()
        is_deployed = state.get("deployed", False)
        vm_status = state.get("vm", {}).get("status", "")

        # If VM is only provisioned (not fully configured), continue with Ansible
        if vm_status == "provisioned":
            logger.log("")
            logger.log("VM is provisioned but not fully configured.")
            logger.log("Continuing with Ansible configuration...")
            logger.log("")
            # Don't skip, continue to Ansible
        elif is_deployed:
            # Compare config hash (same as project state manager)
            last_applied = state.get("last_applied", {})
            last_hash = last_applied.get("config_hash", "")
            current_hash = orch_config.state_manager._calculate_config_hash()

            if current_hash == last_hash:
                logger.success("‚úì No changes detected. Infrastructure is up to date.")
                logger.log("")
                logger.log("Current state:")
                logger.log(f"  ‚Ä¢ VM deployed: {is_deployed}")
                logger.log(f"  ‚Ä¢ IP: {state.get('orchestrator_ip', 'N/A')}")
                logger.log("")
                logger.log("To force re-deployment, use: --force")
                logger.log("To deploy specific addon, use: --addon <name>")
                return
            else:
                logger.log("")
                logger.log("Detected changes in configuration")

                # Try to determine what changed
                last_config = state.get("config", {})

                # Check VM config changes
                if orch_config.config.get("vm") != last_config.get("vm"):
                    logger.log("  ‚Ä¢ VM configuration changed")
                    skip_terraform = False  # Need terraform
                else:
                    logger.log("  ‚Ä¢ VM configuration unchanged")
                    skip_terraform = True  # Skip terraform

                # Check addon configs
                if orch_config.config.get("grafana") != last_config.get("grafana"):
                    logger.log("  ‚Ä¢ Grafana configuration changed")
                if orch_config.config.get("prometheus") != last_config.get(
                    "prometheus"
                ):
                    logger.log("  ‚Ä¢ Prometheus configuration changed")

                logger.log("")
        else:
            logger.log("First deployment detected")
    elif force:
        logger.log("Force mode enabled, running full deployment")
    elif addon:
        logger.log(f"Deploying specific addon: {addon}")

    # Generate and save secrets
    logger.log("Checking secrets...")

    orchestrator_dir = shared_dir / "orchestrator"
    orchestrator_dir.mkdir(parents=True, exist_ok=True)

    # Initialize secrets using secret manager
    secrets = orch_config.initialize_secrets()

    if secrets:
        logger.log("‚úì Secrets verified")
    else:
        logger.log("‚úì Secrets generated")

    # Get GCP config
    gcp_config = orch_config.config.get("gcp", {})
    ssh_config = orch_config.config.get("ssh", {})

    gcp_project_id = gcp_config.get("project_id")
    if not gcp_project_id:
        logger.log_error("gcp.project_id not set in shared/orchestrator/config.yml")
        raise SystemExit(1)

    ssh_key_path = ssh_config.get("public_key_path", "~/.ssh/superdeploy_deploy.pub")

    # Terraform
    if not skip_terraform:
        # First ensure we're on default workspace, then init
        terraform_dir = shared_dir / "terraform"

        # Check if .terraform directory exists
        terraform_state_dir = terraform_dir / ".terraform"
        if terraform_state_dir.exists():
            # Try to switch to default workspace before init
            subprocess.run(
                "terraform workspace select default 2>/dev/null || true",
                shell=True,
                cwd=terraform_dir,
                capture_output=True,
            )

        # Init
        from cli.logger import run_with_progress

        try:
            returncode, stdout, stderr = run_with_progress(
                logger,
                "cd shared/terraform && terraform init -upgrade -migrate-state -input=false -no-color",
                "Initializing Terraform",
                cwd=project_root,
            )

            if returncode != 0:
                logger.log_error("Terraform init failed", context=stderr)
                raise SystemExit(1)

            console.print("  [dim]‚úì Terraform initialized[/dim]")
        except Exception as e:
            logger.log_error("Terraform init failed", context=str(e))
            raise SystemExit(1)

        # Select or create orchestrator workspace (silently)
        from cli.terraform_utils import select_workspace

        try:
            select_workspace("orchestrator", create=True)
        except Exception as e:
            logger.log_error("Workspace setup failed", context=str(e))
            raise SystemExit(1)

        # Generate tfvars
        logger.log("Generating terraform variables")
        tfvars = orch_config.to_terraform_vars(gcp_project_id, ssh_key_path)

        # Preserve IP logic: If preserve_ip is enabled, get current IP from state
        if preserve_ip:
            logger.log("Preserve IP mode enabled - keeping static IP")
            current_ip = orch_config.get_ip()
            if current_ip:
                logger.log(f"Current IP to preserve: {current_ip}")
                # Terraform will use existing IP address by name convention
            else:
                logger.log("No existing IP found, will create new one")

        tfvars_file = (
            project_root / "shared" / "terraform" / "orchestrator.auto.tfvars.json"
        )
        import json

        with open(tfvars_file, "w") as f:
            json.dump(tfvars, f, indent=2)

        logger.log("Terraform vars written to: orchestrator.auto.tfvars.json")

        # Apply
        logger.log("Running terraform apply")
        apply_cmd = "cd shared/terraform && terraform apply -auto-approve -no-color -compact-warnings"

        returncode, stdout, stderr = run_with_progress(
            logger,
            apply_cmd,
            "Provisioning infrastructure (this may take 2-3 minutes)",
            cwd=project_root,
        )

        if returncode != 0:
            logger.log_error("Terraform apply failed", context=stderr)
            raise SystemExit(1)

        console.print("  [dim]‚úì VM provisioned[/dim]")

        # Get outputs
        # Ensure we're in orchestrator workspace
        try:
            select_workspace("orchestrator", create=False)
        except Exception as e:
            logger.log_error("Failed to select orchestrator workspace", context=str(e))
            raise SystemExit(1)

        # Get outputs from orchestrator workspace
        terraform_dir = shared_dir / "terraform"
        result = subprocess.run(
            "terraform output -json -no-color",
            shell=True,
            cwd=terraform_dir,
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            logger.log_error("Failed to get terraform outputs", context=result.stderr)
            raise SystemExit(1)

        import json

        outputs = json.loads(result.stdout)

        orchestrator_ip = (
            outputs.get("vm_public_ips", {}).get("value", {}).get("main-0")
        )

        if not orchestrator_ip:
            logger.log_error("Could not find orchestrator IP in terraform outputs")
            logger.log(f"Available outputs: {outputs}")
            raise SystemExit(1)

        # Save only IP to state (VM provisioned, but not yet configured)
        # Full deployment will be marked after Ansible completes successfully
        state = orch_config.state_manager.load_state()
        state["orchestrator_ip"] = orchestrator_ip
        vm_config_data = orch_config.get_vm_config()
        state["vm"] = {
            "name": vm_config_data.get("name", "orchestrator-main-0"),
            "external_ip": orchestrator_ip,
            "deployed_at": vm_config_data.get("deployed_at"),
            "status": "provisioned",  # Not 'running' yet - Ansible pending
            "machine_type": vm_config_data.get("machine_type"),
            "disk_size": vm_config_data.get("disk_size"),
            "services": vm_config_data.get("services", []),
        }
        state["deployed"] = False  # Not fully deployed yet
        orch_config.state_manager.save_state(state)

        # Wait for SSH
        ssh_key = ssh_config.get("key_path", "~/.ssh/superdeploy_deploy")
        ssh_user = ssh_config.get("user", "superdeploy")

        max_attempts = 18
        for attempt in range(1, max_attempts + 1):
            check_cmd = f"ssh -i {ssh_key} -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes {ssh_user}@{orchestrator_ip} 'sudo -n whoami' 2>&1"
            result = subprocess.run(
                check_cmd, shell=True, capture_output=True, text=True, timeout=10
            )

            if result.returncode == 0 and "root" in result.stdout:
                console.print("  [dim]‚úì VM ready[/dim]")
                break

            if attempt < max_attempts:
                time.sleep(10)
        else:
            logger.warning("VM may not be fully ready, continuing anyway...")
            console.print("  [yellow]‚ö†[/yellow] [dim]VM partially ready[/dim]")

        # Show configuration summary with IP
        console.print(
            f"  [dim]‚úì Configuration ‚Ä¢ Environment ‚Ä¢ Orchestrator (main-0: {orchestrator_ip})[/dim]"
        )

        # Clean SSH known_hosts
        subprocess.run(["ssh-keygen", "-R", orchestrator_ip], capture_output=True)

    else:
        orchestrator_ip = orch_config.get_ip()
        if not orchestrator_ip:
            logger.log_error("Orchestrator IP not found. Deploy with Terraform first.")
            raise SystemExit(1)

        # Show configuration summary with IP (skip-terraform mode)
        console.print(
            f"  [dim]‚úì Configuration ‚Ä¢ Environment ‚Ä¢ Orchestrator (main-0: {orchestrator_ip})[/dim]"
        )

    # Create/Update Ansible inventory file (for both terraform and skip-terraform cases)
    inventory_dir = shared_dir / "ansible" / "inventories"
    inventory_dir.mkdir(parents=True, exist_ok=True)

    inventory_file = inventory_dir / "orchestrator.ini"
    ssh_user = ssh_config.get("user", "superdeploy")
    ssh_key = ssh_config.get("key_path", "~/.ssh/superdeploy_deploy")

    inventory_content = f"""[orchestrator]
orchestrator-main-0 ansible_host={orchestrator_ip} ansible_user={ssh_user} ansible_ssh_private_key_file={ssh_key} ansible_become=yes ansible_become_method=sudo

[all:vars]
ansible_python_interpreter=/usr/bin/python3
"""

    with open(inventory_file, "w") as f:
        f.write(inventory_content)

    # Phase 1 complete - show summary
    logger.success(f"Configuration ‚Ä¢ Secrets ‚Ä¢ VM @ {orchestrator_ip}")

    # Ansible - Phase 2 & 3
    logger.step("[2/3] Base System")

    from cli.ansible_utils import build_ansible_command

    ansible_dir = project_root / "shared" / "ansible"

    # Prepare ansible vars
    ansible_vars = orch_config.to_ansible_vars()

    # Load orchestrator secrets from secrets.yml
    orchestrator_secrets = orch_config.get_secrets()

    # Add secrets to ansible vars (wrap in 'secrets' key to match generate-env.yml expectations)
    ansible_vars["project_secrets"] = {"secrets": orchestrator_secrets}

    # Add orchestrator IP and ansible_host for runtime variables
    ansible_env_vars = {
        "superdeploy_root": str(project_root),
        "orchestrator_ip": orchestrator_ip,
        "ansible_host": orchestrator_ip,  # For addon env variables that use from_ansible
    }

    # Determine ansible tags and enabled addons
    if addon:
        # Deploy only specific addon(s)
        enabled_addons_list = [a.strip() for a in addon.split(",")]
        ansible_tags = "addons"  # Only run addons tag
        logger.log(f"Deploying only addon(s): {', '.join(enabled_addons_list)}")
    elif tags:
        ansible_tags = tags
        # For orchestrator, deploy monitoring addon
        enabled_addons_list = ["monitoring"]
    else:
        ansible_tags = "foundation,addons"
        # Deploy monitoring addon by default
        enabled_addons_list = ["monitoring"]

    logger.log(f"Running ansible with tags: {ansible_tags}")

    ansible_cmd = build_ansible_command(
        ansible_dir=ansible_dir,
        project_root=project_root,
        project_config=ansible_vars,
        env_vars=ansible_env_vars,
        tags=ansible_tags,
        project_name="orchestrator",
        ask_become_pass=False,
        enabled_addons=enabled_addons_list,
        force=force,
    )

    # Run ansible with clean tree view (no messy logs)
    from cli.ansible_runner import AnsibleRunner

    runner = AnsibleRunner(logger, title="Configuring Orchestrator", verbose=verbose)
    result_returncode = runner.run(ansible_cmd, cwd=project_root)

    if result_returncode != 0:
        logger.log_error(
            "Ansible configuration failed", context="Check logs for details"
        )
        console.print(f"\n[dim]Logs saved to:[/dim] {logger.log_path}")
        console.print(
            f"[dim]Ansible detailed log:[/dim] {logger.log_path.parent / f'{logger.log_path.stem}_ansible.log'}\n"
        )
        raise SystemExit(1)

    console.print("[green]‚úì Orchestrator configured[/green]")

    logger.success("Services configured successfully")

    # Mark deployment as complete (Ansible succeeded)
    orch_config.mark_deployed(
        orchestrator_ip,
        vm_config=orch_config.get_vm_config(),
        config=orch_config.config,
    )

    # Display info and credentials (always show, regardless of verbose mode)
    secrets = orch_config.get_secrets()

    grafana_pass = secrets.get("GRAFANA_ADMIN_PASSWORD", "")

    console.print(f"\n[cyan]üìç Orchestrator IP:[/cyan] {orchestrator_ip}")
    console.print("\n[bold cyan]üîê Access Credentials:[/bold cyan]")
    console.print("\n[cyan]üìä Grafana (Monitoring):[/cyan]")
    console.print(f"   URL: http://{orchestrator_ip}:3000")
    console.print("   Username: [bold]admin[/bold]")
    console.print(f"   Password: [bold]{grafana_pass}[/bold]")
    console.print("\n[cyan]üìà Prometheus (Metrics):[/cyan]")
    console.print(f"   URL: http://{orchestrator_ip}:9090")
    console.print("   [dim](No authentication required)[/dim]")

    console.print(f"\n[dim]Logs saved to:[/dim] {logger.log_path}")
    console.print(
        f"[dim]Ansible detailed log:[/dim] {logger.log_path.parent / f'{logger.log_path.stem}_ansible.log'}\n"
    )
