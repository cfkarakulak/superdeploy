"""SuperDeploy CLI - Orchestrator command (Deploy global Forgejo)"""

import click
from rich.console import Console
from rich.panel import Panel
from cli.utils import get_project_root, run_command
from cli.terraform_utils import (
    terraform_init,
    terraform_apply,
    select_workspace,
    get_terraform_outputs,
)
from cli.ansible_utils import build_ansible_command

console = Console()


@click.group()
def orchestrator():
    """Manage global Forgejo orchestrator"""
    pass


@orchestrator.command()
@click.option("--skip-terraform", is_flag=True, help="Skip Terraform (VM already exists)")
@click.option("--preserve-ip", is_flag=True, help="Preserve static IP on destroy (for production)")
def up(skip_terraform, preserve_ip):
    """Deploy orchestrator VM with Forgejo (runs Terraform + Ansible by default)"""
    console.print(
        Panel.fit(
            "[bold cyan]üöÄ Deploying Global Orchestrator[/bold cyan]\n\n"
            "[white]This will deploy a shared Forgejo instance for all projects[/white]",
            border_style="cyan",
        )
    )

    project_root = get_project_root()
    shared_dir = project_root / "shared"

    # Load orchestrator config
    from cli.core.orchestrator_loader import OrchestratorLoader

    orchestrator_loader = OrchestratorLoader(shared_dir)

    try:
        orch_config = orchestrator_loader.load()
    except FileNotFoundError as e:
        console.print(f"[red]‚ùå {e}[/red]")
        raise SystemExit(1)

    # Generate and save secrets FIRST (before any checks)
    console.print("\n[cyan]üîê Checking secrets...[/cyan]")
    import secrets as secrets_module

    orchestrator_dir = shared_dir / "orchestrator"
    orchestrator_dir.mkdir(parents=True, exist_ok=True)
    
    env_file = orchestrator_dir / ".env"
    
    # Check if secrets already exist
    if env_file.exists():
        console.print("[dim]‚úì Using existing orchestrator secrets from .env[/dim]")
        # Load existing secrets
        project_secrets = {}
        with open(env_file, "r") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    project_secrets[key] = value
    else:
        # Generate new secrets
        console.print("[cyan]Generating new orchestrator secrets...[/cyan]")
        project_secrets = {
            "FORGEJO_ADMIN_PASSWORD": secrets_module.token_urlsafe(32),
            "FORGEJO_DB_PASSWORD": secrets_module.token_urlsafe(32),
            "FORGEJO_SECRET_KEY": secrets_module.token_urlsafe(48),  # 64 chars
            "FORGEJO_INTERNAL_TOKEN": secrets_module.token_urlsafe(79),  # 105 chars
        }
        
        # Save secrets to .env file
        env_content = """# =============================================================================
# Orchestrator Secrets
# =============================================================================
# Auto-generated by: superdeploy orchestrator up
# DO NOT COMMIT THIS FILE TO GIT!
# =============================================================================

# Forgejo Admin Password
FORGEJO_ADMIN_PASSWORD={FORGEJO_ADMIN_PASSWORD}

# Forgejo Database Password
FORGEJO_DB_PASSWORD={FORGEJO_DB_PASSWORD}

# Forgejo Secret Key (for encryption)
FORGEJO_SECRET_KEY={FORGEJO_SECRET_KEY}

# Forgejo Internal Token (for API)
FORGEJO_INTERNAL_TOKEN={FORGEJO_INTERNAL_TOKEN}
""".format(**project_secrets)
        
        with open(env_file, "w") as f:
            f.write(env_content)
        
        # Set restrictive permissions (owner read/write only)
        env_file.chmod(0o600)
        
        console.print(f"[green]‚úÖ Orchestrator secrets saved to: shared/orchestrator/.env[/green]")
    
    # Check/create shared config/.env for Docker credentials
    config_dir = shared_dir / "config"
    config_dir.mkdir(parents=True, exist_ok=True)
    config_env_file = config_dir / ".env"
    
    if not config_env_file.exists():
        console.print("[cyan]Creating shared/config/.env for Docker credentials...[/cyan]")
        config_content = """# =============================================================================
# Shared Configuration
# =============================================================================
# This file contains shared configuration used by all projects
# DO NOT COMMIT THIS FILE TO GIT!
# =============================================================================

# Docker Hub Credentials
# These are used by all projects to push Docker images
# Get your token from: https://hub.docker.com/settings/security
DOCKER_REGISTRY=docker.io
DOCKER_ORG=
DOCKER_USERNAME=
DOCKER_TOKEN=
"""
        with open(config_env_file, "w") as f:
            f.write(config_content)
        
        config_env_file.chmod(0o600)
        console.print(f"[green]‚úÖ Shared config created: shared/config/.env[/green]")
        console.print("[yellow]‚ö†Ô∏è  Please edit shared/config/.env and fill in Docker credentials[/yellow]")
    else:
        console.print("[dim]‚úì Using existing shared config from config/.env[/dim]")

    # Get GCP config from orchestrator.yml
    gcp_config = orch_config.config.get('gcp', {})
    ssh_config = orch_config.config.get('ssh', {})
    
    gcp_project_id = gcp_config.get('project_id')
    if not gcp_project_id:
        console.print("[red]‚ùå gcp.project_id not set in shared/orchestrator.yml[/red]")
        raise SystemExit(1)

    ssh_key_path = ssh_config.get('public_key_path', '~/.ssh/superdeploy_deploy.pub')

    console.print("\n[cyan]üìã Orchestrator Configuration:[/cyan]")
    vm_config = orch_config.get_vm_config()
    console.print(f"  GCP Project: {gcp_project_id}")
    console.print(f"  Region: {gcp_config.get('region')}")
    console.print(f"  Zone: {gcp_config.get('zone')}")
    console.print(f"  VM: {vm_config.get('name')}")
    console.print(f"  Machine Type: {vm_config.get('machine_type')}")
    console.print(f"  Disk Size: {vm_config.get('disk_size')}GB")

    # Terraform (skip if flag provided or already deployed)
    if skip_terraform or orch_config.is_deployed():
        console.print("\n[dim]‚òÅÔ∏è  Skipping Terraform (VM already exists)...[/dim]")
        orchestrator_ip = orch_config.get_ip()
        if not orchestrator_ip:
            console.print("[red]‚ùå No IP found! Run without --skip-terraform[/red]")
            raise SystemExit(1)
        console.print(f"[dim]‚úì Using existing IP: {orchestrator_ip}[/dim]")
    else:
        console.print("\n[cyan]‚òÅÔ∏è  Provisioning VM (Terraform)...[/cyan]")

        import subprocess
        terraform_dir = shared_dir / "terraform"
        
        # Create workspace first (non-interactive)
        console.print("[dim]Creating workspace...[/dim]")
        result = subprocess.run(
            ["terraform", "workspace", "new", "orchestrator"],
            cwd=terraform_dir,
            capture_output=True,
            text=True,
        )
        # Ignore error if workspace already exists
        
        # Select workspace
        result = subprocess.run(
            ["terraform", "workspace", "select", "orchestrator"],
            cwd=terraform_dir,
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            console.print(f"[red]‚ùå Failed to select workspace: {result.stderr}[/red]")
            raise SystemExit(1)
        
        console.print("[dim]‚úì Workspace: orchestrator[/dim]")
        
        # Now init
        terraform_init()

        # Generate tfvars
        tfvars = orch_config.to_terraform_vars(gcp_project_id, ssh_key_path)
        tfvars_file = shared_dir / "terraform" / "orchestrator.auto.tfvars.json"

        import json

        with open(tfvars_file, "w") as f:
            json.dump(tfvars, f, indent=2)

        console.print(f"[dim]‚úì Generated: {tfvars_file.name}[/dim]")

        # Apply
        terraform_apply("orchestrator", None, var_file=str(tfvars_file), auto_approve=True)

        console.print("[green]‚úÖ VM provisioned![/green]")

        # Get IP
        outputs = get_terraform_outputs("orchestrator")
        vm_ips = outputs.get("vm_public_ips", {}).get("value", {})
        orchestrator_ip = vm_ips.get("main-0")

    if not orchestrator_ip:
        console.print("[red]‚ùå Failed to get orchestrator IP from Terraform[/red]")
        raise SystemExit(1)

    console.print(f"[green]‚úÖ Orchestrator IP: {orchestrator_ip}[/green]")

    # Wait for SSH to be ready
    console.print("\n[yellow]‚è≥ Waiting for SSH to be ready...[/yellow]")
    import time
    import subprocess
    
    ssh_key = ssh_config.get('key_path', '~/.ssh/superdeploy_deploy')
    ssh_user = ssh_config.get('user', 'superdeploy')
    
    max_attempts = 30
    for attempt in range(1, max_attempts + 1):
        try:
            result = subprocess.run(
                f"ssh -i {ssh_key} -o ConnectTimeout=5 -o StrictHostKeyChecking=no -o BatchMode=yes {ssh_user}@{orchestrator_ip} 'echo ready'",
                shell=True,
                capture_output=True,
                text=True,
                timeout=10,
            )
            
            if result.returncode == 0:
                console.print(f"[green]‚úÖ SSH ready after {attempt * 10}s[/green]")
                break
        except:
            pass
        
        if attempt < max_attempts:
            console.print(f"[dim]Attempt {attempt}/{max_attempts}... waiting 10s[/dim]")
            time.sleep(10)
        else:
            console.print("[yellow]‚ö†Ô∏è  SSH not ready yet, continuing anyway...[/yellow]")
            break

    # Ansible
    console.print("\n[cyan]‚öôÔ∏è  Configuring Forgejo (Ansible)...[/cyan]")

    ansible_dir = shared_dir / "ansible"

    # Generate inventory
    inventory_content = f"""[orchestrator]
orchestrator ansible_host={orchestrator_ip} ansible_user=superdeploy
"""
    inventory_path = ansible_dir / "inventories" / "orchestrator.ini"
    inventory_path.parent.mkdir(parents=True, exist_ok=True)

    with open(inventory_path, "w") as f:
        f.write(inventory_content)

    console.print(f"[dim]‚úì Generated: {inventory_path.name}[/dim]")

    # Get Ansible vars
    ansible_vars = orch_config.to_ansible_vars()
    
    # IMPORTANT: Manually set project_secrets for orchestrator
    # (generate_ansible_extra_vars looks in projects/{name}/.env but orchestrator is in shared/orchestrator/.env)
    ansible_vars["project_secrets"] = project_secrets
    
    # Add preserve_ip flag to Ansible vars if set
    if preserve_ip:
        ansible_vars["preserve_ip"] = True
        console.print("[yellow]‚ö†Ô∏è  IP preservation enabled - static IP will be kept on destroy[/yellow]")

    # Build Ansible command
    ansible_cmd = build_ansible_command(
        ansible_dir=ansible_dir,
        project_root=project_root,
        project_config=ansible_vars,
        env_vars=project_secrets,  # Pass secrets as env vars
        tags="foundation,addons",
        project_name="orchestrator",
    )

    run_command(ansible_cmd)

    console.print("[green]‚úÖ Forgejo configured![/green]")

    # Create Forgejo PAT
    console.print("\n[cyan]üîë Creating Forgejo PAT...[/cyan]")
    
    import urllib.parse
    import time
    import requests
    
    forgejo_config = orch_config.get_forgejo_config()
    admin_user = forgejo_config.get('admin_user')
    admin_password = project_secrets.get('FORGEJO_ADMIN_PASSWORD')
    org = forgejo_config.get('org')
    repo = forgejo_config.get('repo')
    port = forgejo_config.get('port', 3001)
    
    # Wait for Forgejo API to be ready
    console.print("[dim]Waiting for Forgejo API...[/dim]")
    forgejo_url = f"http://{orchestrator_ip}:{port}"
    for attempt in range(30):
        try:
            response = requests.get(f"{forgejo_url}/api/v1/version", timeout=5)
            if response.status_code == 200:
                console.print("[dim]‚úì Forgejo API is ready[/dim]")
                break
        except:
            pass
        if attempt < 29:
            time.sleep(5)
    
    # Create PAT
    token_name = f"superdeploy-pat-{int(time.time())}"
    try:
        response = requests.post(
            f"{forgejo_url}/api/v1/users/{admin_user}/tokens",
            auth=(admin_user, admin_password),
            json={
                "name": token_name,
                "scopes": [
                    "write:activitypub",
                    "write:admin",
                    "write:issue",
                    "write:misc",
                    "write:notification",
                    "write:organization",
                    "write:package",
                    "write:repository",
                    "write:user",
                ],
            },
            timeout=10,
        )
        
        if response.status_code == 201:
            forgejo_pat = response.json()["sha1"]
            console.print("[green]‚úÖ Forgejo PAT created[/green]")
            
            # Save PAT to orchestrator .env
            env_file = shared_dir / "orchestrator" / ".env"
            with open(env_file, "r") as f:
                lines = f.readlines()
            
            # Add or update FORGEJO_PAT
            pat_found = False
            for i, line in enumerate(lines):
                if line.startswith("FORGEJO_PAT="):
                    lines[i] = f"FORGEJO_PAT={forgejo_pat}\n"
                    pat_found = True
                    break
            
            if not pat_found:
                # Just add PAT
                lines.append(f"\n# Forgejo Personal Access Token\n")
                lines.append(f"FORGEJO_PAT={forgejo_pat}\n")
            
            with open(env_file, "w") as f:
                f.writelines(lines)
            
            console.print("[green]‚úÖ PAT saved to shared/orchestrator/.env[/green]")
        else:
            console.print(f"[yellow]‚ö†Ô∏è  PAT creation failed: {response.text}[/yellow]")
            forgejo_pat = None
    except Exception as e:
        console.print(f"[yellow]‚ö†Ô∏è  PAT creation failed: {e}[/yellow]")
        forgejo_pat = None
    
    # Push superdeploy repo to Forgejo
    console.print("\n[cyan]üì§ Pushing superdeploy repo to Forgejo...[/cyan]")
    
    # Wait for Forgejo to be fully ready
    console.print("[dim]Waiting for Forgejo to be ready...[/dim]")
    for attempt in range(30):
        try:
            result = subprocess.run(
                f"curl -sSf -m 3 http://{orchestrator_ip}:{port}/ >/dev/null 2>&1",
                shell=True,
                capture_output=True,
            )
            if result.returncode == 0:
                console.print("[dim]‚úì Forgejo is ready[/dim]")
                break
        except:
            pass
        if attempt < 29:
            time.sleep(5)
    
    # Build Forgejo URL with credentials
    encoded_pass = urllib.parse.quote(admin_password)
    forgejo_url = f"http://{admin_user}:{encoded_pass}@{orchestrator_ip}:{port}/{org}/{repo}.git"
    
    # Remove old forgejo remote if exists
    subprocess.run(
        ["git", "remote", "remove", "forgejo"],
        cwd=str(project_root),
        capture_output=True,
    )
    
    # Add new forgejo remote
    result = subprocess.run(
        ["git", "remote", "add", "forgejo", forgejo_url],
        cwd=str(project_root),
        capture_output=True,
        text=True,
    )
    
    if result.returncode != 0:
        console.print(f"[yellow]‚ö†Ô∏è  Failed to add forgejo remote: {result.stderr}[/yellow]")
    else:
        # Push to forgejo
        result = subprocess.run(
            ["git", "push", "forgejo", "master:master", "-f"],
            cwd=str(project_root),
            capture_output=True,
            text=True,
        )
        
        if result.returncode == 0:
            console.print("[green]‚úÖ Superdeploy repo pushed to Forgejo![/green]")
        else:
            console.print(f"[yellow]‚ö†Ô∏è  Git push failed: {result.stderr}[/yellow]")

    # Mark as deployed
    orch_config.mark_deployed(orchestrator_ip)

    console.print("\n[green]‚úÖ Orchestrator deployed successfully![/green]")
    console.print(f"\n[bold]Forgejo URL:[/bold] http://{orchestrator_ip}:3001")
    console.print(f"[bold]Admin User:[/bold] {admin_user}")
    console.print(f"[bold]Admin Password:[/bold] {admin_password}")
    console.print(f"[bold]Repository:[/bold] http://{orchestrator_ip}:{port}/{org}/{repo}")
    console.print("\n[bold]üìù Next steps:[/bold]")
    console.print("1. Deploy your projects: superdeploy up -p <project>")


@orchestrator.command()
@click.option("--yes", "-y", is_flag=True, help="Skip confirmation prompt")
@click.option("--preserve-ip", is_flag=True, help="Keep static IP (don't delete)")
def down(yes, preserve_ip):
    """Destroy orchestrator VM and clean up state"""
    from rich.prompt import Confirm
    
    console.print(
        Panel.fit(
            "[bold red]‚ö†Ô∏è  Orchestrator Destruction[/bold red]\n\n"
            "[white]This will destroy the orchestrator VM and clean up all state[/white]",
            border_style="red",
        )
    )
    
    if not yes:
        confirmed = Confirm.ask(
            "[bold red]Are you sure you want to destroy orchestrator?[/bold red]",
            default=False
        )
        if not confirmed:
            console.print("[yellow]‚ùå Cancelled[/yellow]")
            return
    
    project_root = get_project_root()
    shared_dir = project_root / "shared"
    
    from cli.core.orchestrator_loader import OrchestratorLoader
    
    orchestrator_loader = OrchestratorLoader(shared_dir)
    
    try:
        orch_config = orchestrator_loader.load()
    except FileNotFoundError as e:
        console.print(f"[red]‚ùå {e}[/red]")
        raise SystemExit(1)
    
    console.print("\n[cyan]üî• Destroying orchestrator...[/cyan]")
    
    import subprocess
    
    # Use Terraform destroy (same as superdeploy down)
    console.print("\n[cyan]üî• Running terraform destroy...[/cyan]")
    
    terraform_success = False
    
    # Check if workspace exists
    terraform_state_dir = shared_dir / "terraform" / "terraform.tfstate.d" / "orchestrator"
    if not terraform_state_dir.exists():
        console.print("[dim]‚úì No Terraform state found[/dim]")
    else:
        try:
            terraform_init()
            select_workspace("orchestrator", create=False)
            
            # Generate tfvars for destroy
            gcp_config = orch_config.config.get('gcp', {})
            ssh_config = orch_config.config.get('ssh', {})
            gcp_project_id = gcp_config.get('project_id')
            ssh_key_path = ssh_config.get('public_key_path', '~/.ssh/superdeploy_deploy.pub')
            
            tfvars = orch_config.to_terraform_vars(gcp_project_id, ssh_key_path)
            tfvars_file = shared_dir / "terraform" / "orchestrator.auto.tfvars.json"
            
            import json
            with open(tfvars_file, "w") as f:
                json.dump(tfvars, f, indent=2)
            
            # Run destroy
            terraform_dir = shared_dir / "terraform"
            result = subprocess.run(
                ["terraform", "destroy", f"-var-file={tfvars_file}", "-auto-approve"],
                cwd=terraform_dir,
                capture_output=False,
                text=True,
            )
            
            if result.returncode == 0:
                console.print("[green]‚úÖ Terraform destroy successful[/green]")
                terraform_success = True
            else:
                console.print("[yellow]‚ö†Ô∏è  Terraform destroy had issues, cleaning manually...[/yellow]")
                
        except Exception as e:
            console.print(f"[yellow]‚ö†Ô∏è  Terraform destroy error: {e}[/yellow]")
            console.print("[yellow]Cleaning manually...[/yellow]")
    
    # Manual cleanup with gcloud (always run to ensure everything is gone)
    console.print("\n[cyan]üßπ Manual cleanup with gcloud...[/cyan]")
    
    gcp_config = orch_config.config.get('gcp', {})
    zone = gcp_config.get('zone', 'us-central1-a')
    region = gcp_config.get('region', 'us-central1')
    
    # Always try to clean up, even if terraform succeeded
    if True:
        console.print("\n[cyan]üßπ Manual cleanup with gcloud...[/cyan]")
        
        gcp_config = orch_config.config.get('gcp', {})
        zone = gcp_config.get('zone', 'us-central1-a')
        region = gcp_config.get('region', 'us-central1')
        
        # Delete VM
        console.print("[dim]Deleting VM orchestrator-main-0...[/dim]")
        result = subprocess.run(
            f"gcloud compute instances delete orchestrator-main-0 --zone={zone} --quiet",
            shell=True,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            console.print("[green]  ‚úì VM deleted[/green]")
        elif "not found" in result.stderr.lower() or "could not fetch resource" in result.stderr.lower():
            console.print("[dim]  ‚úì VM not found (already deleted)[/dim]")
        else:
            console.print(f"[yellow]  ‚ö† VM deletion: {result.stderr.strip()[:100]}[/yellow]")
        
        # Delete External IP (unless --preserve-ip flag is set)
        if preserve_ip:
            console.print("[yellow]  ‚äô External IP preserved (--preserve-ip flag)[/yellow]")
        else:
            console.print("[dim]Deleting External IP orchestrator-main-0-ip...[/dim]")
            result = subprocess.run(
                f"gcloud compute addresses delete orchestrator-main-0-ip --region={region} --quiet",
                shell=True,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                console.print("[green]  ‚úì External IP deleted[/green]")
            elif "not found" in result.stderr.lower() or "could not fetch resource" in result.stderr.lower():
                console.print("[dim]  ‚úì External IP not found (already deleted)[/dim]")
            else:
                console.print(f"[yellow]  ‚ö† External IP deletion: {result.stderr.strip()[:100]}[/yellow]")
        
        # Delete Firewall Rules (all network rules)
        console.print("[dim]Deleting Firewall Rules...[/dim]")
        result = subprocess.run(
            "gcloud compute firewall-rules list --filter='network:superdeploy-network' --format='value(name)'",
            shell=True,
            capture_output=True,
            text=True,
        )
        
        if result.returncode == 0 and result.stdout.strip():
            firewall_rules = result.stdout.strip().split('\n')
            for rule in firewall_rules:
                rule = rule.strip()
                if rule:
                    result = subprocess.run(
                        f"gcloud compute firewall-rules delete {rule} --quiet",
                        shell=True,
                        capture_output=True,
                        text=True,
                    )
                    if result.returncode == 0:
                        console.print(f"[green]  ‚úì {rule} deleted[/green]")
                    else:
                        console.print(f"[yellow]  ‚ö† {rule} deletion failed[/yellow]")
        else:
            console.print("[dim]  ‚úì No firewall rules found[/dim]")
        
        # Delete Subnet
        console.print("[dim]Deleting Subnet superdeploy-network-subnet...[/dim]")
        result = subprocess.run(
            f"gcloud compute networks subnets delete superdeploy-network-subnet --region={region} --quiet",
            shell=True,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            console.print("[green]  ‚úì Subnet deleted[/green]")
        elif "not found" in result.stderr.lower() or "could not fetch resource" in result.stderr.lower():
            console.print("[dim]  ‚úì Subnet not found (already deleted)[/dim]")
        else:
            console.print(f"[yellow]  ‚ö† Subnet deletion: {result.stderr.strip()[:100]}[/yellow]")
        
        # Delete Network
        console.print("[dim]Deleting Network superdeploy-network...[/dim]")
        result = subprocess.run(
            "gcloud compute networks delete superdeploy-network --quiet",
            shell=True,
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            console.print("[green]  ‚úì Network deleted[/green]")
        elif "not found" in result.stderr.lower() or "could not fetch resource" in result.stderr.lower():
            console.print("[dim]  ‚úì Network not found (already deleted)[/dim]")
        else:
            console.print(f"[yellow]  ‚ö† Network deletion: {result.stderr.strip()[:100]}[/yellow]")
        
        console.print("[green]‚úÖ Manual cleanup complete[/green]")
    
    # 2. Clean Terraform state
    terraform_state_dir = shared_dir / "terraform" / "terraform.tfstate.d" / "orchestrator"
    if terraform_state_dir.exists():
        import shutil
        shutil.rmtree(terraform_state_dir)
        console.print("[green]‚úÖ Terraform state cleaned[/green]")
    
    # 3. Reset config state
    orch_config.config["state"] = {
        "deployed": False,
        "ip": "",
        "last_updated": "",
    }
    
    import yaml
    with open(orch_config.config_path, "w") as f:
        yaml.dump(orch_config.config, f, default_flow_style=False, sort_keys=False)
    
    console.print("[green]‚úÖ Config state reset[/green]")
    
    # 4. Clean inventory
    inventory_file = shared_dir / "ansible" / "inventories" / "orchestrator.ini"
    if inventory_file.exists():
        inventory_file.unlink()
        console.print("[green]‚úÖ Inventory cleaned[/green]")
    
    console.print("\n[green]‚úÖ Orchestrator destroyed and cleaned up![/green]")


@orchestrator.command()
def status():
    """Show orchestrator status"""
    project_root = get_project_root()
    shared_dir = project_root / "shared"

    from cli.core.orchestrator_loader import OrchestratorLoader

    orchestrator_loader = OrchestratorLoader(shared_dir)

    try:
        orch_config = orchestrator_loader.load()
    except FileNotFoundError as e:
        console.print(f"[red]‚ùå {e}[/red]")
        raise SystemExit(1)

    if orch_config.is_deployed():
        console.print("[green]‚úÖ Orchestrator is deployed[/green]")
        console.print(f"  IP: {orch_config.get_ip()}")
        console.print(f"  URL: http://{orch_config.get_ip()}:3001")
        console.print(
            f"  Last Updated: {orch_config.config.get('state', {}).get('last_updated', 'Unknown')}"
        )
    else:
        console.print("[yellow]‚ö†Ô∏è  Orchestrator not deployed[/yellow]")
        console.print("  Run: superdeploy orchestrator up")





if __name__ == "__main__":
    orchestrator()
