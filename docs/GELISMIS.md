# ðŸŽ¯ SuperDeploy - GeliÅŸmiÅŸ Konular

## Secret Encryption (AGE)

SuperDeploy, environment variable'larÄ± GitHub Actions'tan Forgejo runner'a gÃ¼venli ÅŸekilde transfer etmek iÃ§in AGE encryption kullanÄ±r.

### NasÄ±l Ã‡alÄ±ÅŸÄ±r

**1. Setup (Tek Seferlik):**
- Ansible, Forgejo runner VM'inde AGE keypair oluÅŸturur
- Private key VM'de kalÄ±r (`/opt/forgejo-runner/.age/key.txt`)
- Public key `superdeploy sync` ile GitHub Secrets'a eklenir

**2. Deployment:**
```
GitHub Actions
  â†“
  1. .env dosyasÄ± oluÅŸturulur (secrets'tan)
  2. AGE public key ile ÅŸifrelenir
  3. Base64 encode edilir
  â†“
Forgejo API
  â†“
Forgejo Runner
  â†“
  4. Base64 decode edilir
  5. AGE private key ile ÅŸifresi Ã§Ã¶zÃ¼lÃ¼r
  6. /opt/superdeploy/.env.decrypted yazÄ±lÄ±r
  â†“
Docker Compose
  â†“
  7. Åžifreli .env yÃ¼klenir
  8. Services baÅŸlatÄ±lÄ±r
  â†“
Cleanup
  â†“
  9. /opt/superdeploy/.env.decrypted gÃ¼venli ÅŸekilde silinir (shred -u)
```

### GÃ¼venlik Ã–zellikleri

**Encryption at Rest:**
Private key sadece runner VM'de, disk encrypted olarak saklanÄ±r.

**Encryption in Transit:**
Environment variable'lar GitHub â†’ Forgejo transfer'i sÄ±rasÄ±nda ÅŸifreli gider.

**No Persistent Storage:**
Åžifresi Ã§Ã¶zÃ¼lmÃ¼ÅŸ .env dosyasÄ± sadece deployment sÃ¼resince disk'te kalÄ±r, sonra `shred` ile gÃ¼venli ÅŸekilde silinir.

**Access Control:**
Private key'e sadece `superdeploy` user'Ä± (runner) eriÅŸebilir. File permissions: `600`.

## Zero-Downtime Deployment

SuperDeploy, Docker Compose'un update stratejisiyle zero-downtime deployment yapar.

### Deployment AkÄ±ÅŸÄ±

**1. Health Check:**
Yeni container ayaÄŸa kalkar ve health check'leri geÃ§er.

**2. Graceful Shutdown:**
Eski container'a SIGTERM gÃ¶nderilir. `stop_grace_period` kadar bekler.

**3. Traffic Switch:**
Load balancer (Caddy) yeni container'a yÃ¶nlendirmeye baÅŸlar.

**4. Old Container Cleanup:**
Eski container temizlenir.

### Config (docker-compose.apps.yml)

```yaml
api:
  healthcheck:
    test: ["CMD", "curl", "-fsS", "http://localhost:8000/healthz"]
    interval: 10s
    timeout: 5s
    retries: 12
    start_period: 30s
  stop_grace_period: 30s  # Graceful shutdown
```

**Health Check Parameters:**
- `start_period`: Container baÅŸlangÄ±cÄ±nda bekle (30s)
- `interval`: Her 10 saniyede bir kontrol et
- `retries`: 12 baÅŸarÄ±sÄ±z denemeden sonra unhealthy say
- `timeout`: Her check max 5 saniye

**Graceful Shutdown:**
`stop_grace_period` iÃ§inde container ÅŸunlarÄ± yapmalÄ±:
- Yeni request'leri reddet
- Mevcut request'leri tamamla
- Database connection'larÄ± temizle
- Temiz ÅŸekilde kapat

### Application Health Endpoint

UygulamanÄ±zda `/health` veya `/healthz` endpoint'i olmalÄ±:

```python
# FastAPI Ã¶rneÄŸi
@app.get("/healthz")
async def health_check():
    try:
        # Database baÄŸlantÄ±sÄ±nÄ± kontrol et
        await db.execute("SELECT 1")
        return {"status": "healthy"}
    except Exception as e:
        raise HTTPException(status_code=503, detail="unhealthy")
```

## Database Migrations

### Otomatik Migration

Deployment sÄ±rasÄ±nda migration Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
superdeploy deploy --app api --tag v1.2.3 --migrate
```

Veya GitHub Actions workflow'unda:

```yaml
inputs:
  migrate:
    description: 'Run DB migrations (true/false)'
    required: false
    default: 'false'
```

### Migration Stratejisi

**Backward Compatible Migrations:**
Her zaman backward compatible migration yazÄ±n. BÃ¶ylece rollback sorun Ã§Ä±karmaz.

**KÃ¶tÃ¼ Ã–rnek:**
```python
# KÃ–TÃœ: Column silme
op.drop_column('users', 'old_field')
```

**Ä°yi Ã–rnek:**
```python
# Ä°YÄ°: Ä°ki aÅŸamalÄ± yaklaÅŸÄ±m
# Migration 1: Column'Ä± nullable yap, kod'da kullanmayÄ± bÄ±rak
op.alter_column('users', 'old_field', nullable=True)

# Deployment 1: Yeni kod deploy et (old_field kullanmÄ±yor)

# Migration 2 (sonraki release): Column'Ä± sil
op.drop_column('users', 'old_field')
```

### Migration Rollback

Migration baÅŸarÄ±sÄ±z olursa deployment durur:

```yaml
- name: ðŸ—„ï¸ Run DB migrations
  run: |
    if docker compose run --rm api alembic upgrade head; then
      echo "âœ… Migrations completed"
    else
      echo "âŒ MIGRATION FAILED!"
      exit 1
    fi
```

**Manuel Rollback:**

```bash
# Son migration'Ä± geri al
superdeploy run api "alembic downgrade -1"

# Belirli revision'a geri dÃ¶n
superdeploy run api "alembic downgrade abc123"

# Mevcut durumu gÃ¶rÃ¼ntÃ¼le
superdeploy run api "alembic current"

# Migration geÃ§miÅŸi
superdeploy run api "alembic history"
```

## Multi-Environment Setup

Staging ve Production ortamlarÄ± iÃ§in ayrÄ± konfigÃ¼rasyon.

### Repository Branch Strategy

```
main (development)
  â†“
staging (pre-production tests)
  â†“
production (live)
```

### Environment-Specific .env

Her environment iÃ§in ayrÄ± `.env` dosyasÄ±:

```
superdeploy/.env.staging
superdeploy/.env.production
```

### GitHub Environments

Her repo iÃ§in hem `staging` hem `production` environment'Ä± oluÅŸturun:

```bash
# GitHub UI â†’ Settings â†’ Environments â†’ New environment

# Staging: Auto-deploy on push
# Production: Require review before deploy
```

### Workflow DeÄŸiÅŸikliÄŸi

GitHub Actions'ta environment dinamik olarak seÃ§ilir:

```yaml
environment: ${{ github.ref == 'refs/heads/production' && 'production' || 'staging' }}
```

## Custom Domain ve SSL/TLS

### Domain AyarlarÄ±

**1. DNS KonfigÃ¼rasyonu:**
```
A Record:  yourdomain.com        â†’ CORE_EXTERNAL_IP
A Record:  *.yourdomain.com      â†’ CORE_EXTERNAL_IP
CNAME:     api.yourdomain.com    â†’ yourdomain.com
CNAME:     dashboard.yourdomain.com â†’ yourdomain.com
```

**2. Caddy KonfigÃ¼rasyonu:**

```caddyfile
# /opt/superdeploy/Caddyfile

yourdomain.com {
    reverse_proxy superdeploy-dashboard:3000
    tls admin@yourdomain.com
}

api.yourdomain.com {
    reverse_proxy superdeploy-api:8000
    tls admin@yourdomain.com
}
```

**3. Caddy Restart:**
```bash
ssh -i ~/.ssh/superdeploy_deploy superdeploy@CORE_IP \
  "docker restart superdeploy-caddy"
```

Caddy otomatik olarak Let's Encrypt SSL sertifikasÄ± alÄ±r.

## Backup ve Restore

### Database Backup

**Otomatik Backup (Cron):**

```bash
# VM'de cron job ekle
ssh -i ~/.ssh/superdeploy_deploy superdeploy@CORE_IP

# Backup scripti oluÅŸtur
cat > /opt/superdeploy/backup-db.sh << 'EOF'
#!/bin/bash
DATE=$(date +%Y%m%d-%H%M%S)
BACKUP_DIR=/opt/backups/postgres
mkdir -p $BACKUP_DIR

docker exec superdeploy-postgres pg_dump \
  -U superdeploy superdeploy_db | \
  gzip > $BACKUP_DIR/backup-$DATE.sql.gz

# Son 7 gÃ¼nÃ¼ tut
find $BACKUP_DIR -name "backup-*.sql.gz" -mtime +7 -delete

echo "âœ… Backup completed: backup-$DATE.sql.gz"
EOF

chmod +x /opt/superdeploy/backup-db.sh

# GÃ¼nlÃ¼k 3:00'de Ã§alÄ±ÅŸacak cron
crontab -e
# 0 3 * * * /opt/superdeploy/backup-db.sh >> /var/log/backup.log 2>&1
```

**Manuel Backup:**

```bash
superdeploy run postgres "pg_dump -U superdeploy superdeploy_db" > backup.sql
```

### Database Restore

```bash
# Backup'Ä± VM'e kopyala
scp -i ~/.ssh/superdeploy_deploy backup.sql superdeploy@CORE_IP:/tmp/

# Restore
ssh -i ~/.ssh/superdeploy_deploy superdeploy@CORE_IP \
  "docker exec -i superdeploy-postgres psql -U superdeploy superdeploy_db < /tmp/backup.sql"
```

### GCS Backup (Ã–nerilen)

Google Cloud Storage'a otomatik backup:

```bash
# GCS bucket oluÅŸtur
gsutil mb -p YOUR_PROJECT_ID gs://superdeploy-backups

# Backup scripti gÃ¼ncelle
cat > /opt/superdeploy/backup-db.sh << 'EOF'
#!/bin/bash
DATE=$(date +%Y%m%d-%H%M%S)
BACKUP_FILE=backup-$DATE.sql.gz

docker exec superdeploy-postgres pg_dump \
  -U superdeploy superdeploy_db | \
  gzip > /tmp/$BACKUP_FILE

# GCS'e yÃ¼kle
gsutil cp /tmp/$BACKUP_FILE gs://superdeploy-backups/

# Local'i temizle
rm /tmp/$BACKUP_FILE

echo "âœ… Backup uploaded to GCS: $BACKUP_FILE"
EOF
```

## Performance Optimization

### 1. Docker Build Caching

Multi-stage build kullanarak dependency layer'Ä±nÄ± cache'leyin:

```dockerfile
# Stage 1: Dependencies
FROM python:3.11-slim as dependencies
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Stage 2: Application
FROM python:3.11-slim
WORKDIR /app
COPY --from=dependencies /usr/local /usr/local
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0"]
```

### 2. Database Connection Pooling

Her request'te yeni connection aÃ§mak yerine pool kullanÄ±n:

```python
# SQLAlchemy
engine = create_engine(
    DATABASE_URL,
    pool_size=20,           # Connection pool boyutu
    max_overflow=10,        # Pool doluysa max ek connection
    pool_pre_ping=True,     # Dead connection'larÄ± tespit et
    pool_recycle=3600       # 1 saatte bir connection'larÄ± yenile
)
```

### 3. Redis Caching

SÄ±k eriÅŸilen data iÃ§in Redis cache:

```python
@app.get("/api/users/{user_id}")
async def get_user(user_id: int):
    # Ã–nce cache'e bak
    cached = await redis.get(f"user:{user_id}")
    if cached:
        return json.loads(cached)
    
    # Cache miss: DB'den Ã§ek
    user = await db.get_user(user_id)
    
    # Cache'e yaz (TTL: 5 dakika)
    await redis.setex(f"user:{user_id}", 300, json.dumps(user))
    
    return user
```

### 4. Horizontal Scaling

Traffic artÄ±ÅŸÄ±nda API replica sayÄ±sÄ±nÄ± artÄ±rÄ±n:

```bash
# Normal trafik: 2 replica
superdeploy scale api=2

# YoÄŸun saatler: 5 replica
superdeploy scale api=5

# Gece dÃ¼ÅŸÃ¼k trafik: 1 replica
superdeploy scale api=1
```

## Security Hardening

### 1. Firewall Rules

Sadece gerekli portlarÄ± aÃ§Ä±k tutun:

```hcl
# terraform/main.tf
resource "google_compute_firewall" "allow_http" {
  name    = "allow-http"
  network = "default"
  
  allow {
    protocol = "tcp"
    ports    = ["80", "443"]  # Sadece HTTP/HTTPS
  }
  
  source_ranges = ["0.0.0.0/0"]  # Herkese aÃ§Ä±k
}

resource "google_compute_firewall" "allow_ssh" {
  name    = "allow-ssh"
  network = "default"
  
  allow {
    protocol = "tcp"
    ports    = ["22"]
  }
  
  source_ranges = ["YOUR_OFFICE_IP/32"]  # Sadece ofis IP'si
}
```

### 2. Container Security

Non-root user kullanÄ±n:

```dockerfile
FROM python:3.11-slim

# User oluÅŸtur
RUN useradd -m -u 1000 appuser

WORKDIR /app
COPY . .

# Ownership deÄŸiÅŸtir
RUN chown -R appuser:appuser /app

# Non-root user'a geÃ§
USER appuser

CMD ["uvicorn", "app.main:app"]
```

### 3. Secret Rotation

DÃ¼zenli olarak passwordleri deÄŸiÅŸtirin:

```bash
# Yeni password oluÅŸtur
NEW_PASSWORD=$(openssl rand -base64 32)

# .env'de gÃ¼ncelle
sed -i "s/POSTGRES_PASSWORD=.*/POSTGRES_PASSWORD=$NEW_PASSWORD/" .env

# GitHub'a sync et
superdeploy sync

# PostgreSQL'de de deÄŸiÅŸtir
superdeploy run postgres "psql -U postgres -c \"ALTER USER superdeploy WITH PASSWORD '$NEW_PASSWORD';\""

# Servisleri restart et
superdeploy restart -a api
```

### 4. HTTPS Only

Caddy'de HTTP'yi HTTPS'e yÃ¶nlendir:

```caddyfile
http://yourdomain.com {
    redir https://yourdomain.com{uri}
}

https://yourdomain.com {
    reverse_proxy superdeploy-dashboard:3000
    
    # Security headers
    header {
        Strict-Transport-Security "max-age=31536000;"
        X-Content-Type-Options "nosniff"
        X-Frame-Options "DENY"
        X-XSS-Protection "1; mode=block"
    }
}
```

## Monitoring ve Alerting

### Prometheus + Grafana Setup

**1. docker-compose.monitoring.yml ekleyin:**

```yaml
services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
  
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
```

**2. prometheus.yml:**

```yaml
scrape_configs:
  - job_name: 'api'
    static_configs:
      - targets: ['superdeploy-api:8000']
  
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
```

### Email Alerting

Deployment notification'lar zaten email olarak geliyor. Critical error'lar iÃ§in de ekleyin:

```python
# Application code
import smtplib

def send_alert(subject, message):
    smtp = smtplib.SMTP('smtp.gmail.com', 587)
    smtp.starttls()
    smtp.login(ALERT_EMAIL, ALERT_PASSWORD)
    smtp.sendmail(ALERT_EMAIL, ALERT_EMAIL, f"Subject: {subject}\n\n{message}")
    smtp.quit()

# KullanÄ±m
try:
    critical_operation()
except Exception as e:
    send_alert("ðŸš¨ Critical Error", str(e))
    raise
```

## Troubleshooting Recipes

### Out of Memory (OOM)

Container memory limiti yetersiz:

```yaml
# docker-compose.apps.yml
api:
  deploy:
    resources:
      limits:
        memory: 2G      # 1G'den 2G'ye Ã§Ä±kar
      reservations:
        memory: 512M
```

### Slow Database Queries

PostgreSQL slow query log'u aktif edin:

```bash
superdeploy run postgres "psql -U postgres -c \"ALTER SYSTEM SET log_min_duration_statement = 1000;\""
superdeploy restart -a postgres

# LoglarÄ± izle
superdeploy logs -a postgres | grep "duration:"
```

### Connection Pool Exhaustion

Pool size'Ä± artÄ±rÄ±n:

```python
# config.py
DATABASE_POOL_SIZE = 50  # 20'den 50'ye Ã§Ä±kar
DATABASE_MAX_OVERFLOW = 20
```

### High CPU Usage

Profiling yapÄ±n:

```bash
# cProfile ile profil Ã§Ä±kar
superdeploy run api "python -m cProfile -o profile.stats app/main.py"

# py-spy ile canlÄ± profiling
pip install py-spy
py-spy top --pid $(docker inspect -f '{{.State.Pid}}' superdeploy-api)
```

---

**SonuÃ§:**

Bu geliÅŸmiÅŸ konular, SuperDeploy'u production-ready hale getirir. GÃ¼venlik, performans, monitoring ve disaster recovery stratejileri sayesinde kurumsal dÃ¼zeyde kullanÄ±ma hazÄ±rsÄ±nÄ±z.

