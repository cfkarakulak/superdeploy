version: '3.8'

networks:
  superdeploy:
    driver: bridge

services:
  # Worker 1 - Scraper with Playwright browsers
  worker-1:
    image: superdeploy/worker:1.0.0  # Pinned version
    container_name: superdeploy-worker-1
    restart: unless-stopped
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - RABBITMQ_URL=${RABBITMQ_URL}
      - PROXY_REGISTRY_URL=${PROXY_REGISTRY_URL}
      - WORKER_ID=worker-1
      - WORKER_CONCURRENCY=${WORKER_CONCURRENCY}
      - WORKER_BROWSER_COUNT=${WORKER_BROWSER_COUNT}
      - WORKER_TIMEOUT=${WORKER_TIMEOUT}
      - BROWSER_HEADLESS=${BROWSER_HEADLESS}
      - BROWSER_ARGS=${BROWSER_ARGS}
      - ENVIRONMENT=${ENVIRONMENT}
      - LOG_LEVEL=${LOG_LEVEL}
    volumes:
      - ./browser_cache:/app/browser_cache
    networks:
      - superdeploy
    # Share IPC namespace for better browser performance
    ipc: host
    # Increase shared memory for Chrome/Playwright
    shm_size: 2gb
    deploy:
      resources:
        limits:
          cpus: '3.5'
          memory: 12G
        reservations:
          cpus: '2'
          memory: 6G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"

  # Worker 2 - Second scraper instance (optional, can be scaled)
  worker-2:
    image: superdeploy/worker:latest
    container_name: superdeploy-worker-2
    restart: always
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - RABBITMQ_URL=${RABBITMQ_URL}
      - PROXY_REGISTRY_URL=${PROXY_REGISTRY_URL}
      - WORKER_ID=worker-2
      - WORKER_CONCURRENCY=${WORKER_CONCURRENCY}
      - WORKER_BROWSER_COUNT=${WORKER_BROWSER_COUNT}
      - WORKER_TIMEOUT=${WORKER_TIMEOUT}
      - BROWSER_HEADLESS=${BROWSER_HEADLESS}
      - BROWSER_ARGS=${BROWSER_ARGS}
      - ENVIRONMENT=${ENVIRONMENT}
      - LOG_LEVEL=${LOG_LEVEL}
    volumes:
      - ./browser_cache:/app/browser_cache
    networks:
      - superdeploy
    ipc: host
    shm_size: 2gb
    deploy:
      resources:
        limits:
          cpus: '3.5'
          memory: 12G
        reservations:
          cpus: '2'
          memory: 6G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 10s
      retries: 3
    # Comment this out to enable worker-2
    profiles:
      - scaling

# Notes:
# - Each worker runs 2-3 browser instances
# - Total: 2 workers Ã— 2-3 browsers = 4-6 parallel browsers
# - Adjust WORKER_CONCURRENCY based on VM resources
# - Use docker compose --profile scaling up to enable worker-2

